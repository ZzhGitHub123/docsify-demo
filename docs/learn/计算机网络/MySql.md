[TOC]

# 架构

# 基础

​		我们的`MySQL`服务器程序和客户端程序本质上都算是计算机上的一个`进程`，这个代表着`MySQL`服务器程序的进程也被称为`MySQL数据库实例`，简称`数据库实例`。

## mysql的bin文件

​		在`MySQL`的安装目录下有一个特别特别重要的`bin`目录，这个目录下存放着许多可执行文件，以`macOS`系统为例，这个`bin`目录的绝对路径就是（在我的机器上）：

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170636.png" alt="image-20210312153927927" style="zoom:70%;" />

## 启动MySQL服务器程序

- 使用可执行文件的相对／绝对路径

  假设我们现在所处的工作目录是`MySQL`的安装目录，也就是`/usr/local/mysql`，我们想启动`bin`目录下的`mysqld`这个可执行文件，可以使用相对路径来启动：

  ```
  ./bin/mysqld
  ```

  或者直接输入`mysqld`的绝对路径也可以：

  ```
  /usr/local/mysql/bin/mysqld
  ```

- 将该`bin`目录的路径加入到环境变量`PATH`中

  如果我们觉得每次执行一个文件都要输入一串长长的路径名贼麻烦的话，可以把该`bin`目录所在的路径添加到环境变量`PATH`中。环境变量`PATH`是一系列路径的集合，各个路径之间使用冒号`:`隔离开，比方说我的机器上的环境变量`PATH`的值就是：

  ```
  /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin
  ```

  我的系统中这个环境变量`PATH`的值表明：当我在输入一个命令时，系统便会在`/usr/local/bin`、`/usr/bin:`、`/bin:`、`/usr/sbin`、`/sbin`这些目录下依次寻找是否存在我们输入的那个命令，如果寻找成功，则执行该目录下对应的可执行文件。所以我们现在可以修改一下这个环境变量`PATH`，把`MySQL`安装目录下的`bin`目录的路径也加入到`PATH`中，在我的机器上修改后的环境变量`PATH`的值为：

  ```
  /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/mysql/bin
  ```

  这样现在不论我们所处的工作目录是啥，我们都可以直接输入可执行文件的名字就可以启动它，比如这样：

  ```
  mysqld
  ```

### UNIX里启动服务器程序

在类`UNIX`系统中用来启动`MySQL`服务器程序的可执行文件有很多，大多在`MySQL`安装目录的`bin`目录下，我们一起来瞅瞅。

#### mysqld

​		`mysqld`这个可执行文件就代表着`MySQL`服务器程序，运行这个可执行文件就可以直接启动一个服务器进程。但这个命令不常用。

#### mysqld_safe

​		`				mysqld_safe`是一个启动脚本，它会间接的调用`mysqld`，而且还顺便启动了另外一个监控进程，这个监控进程在服务器进程挂了的时候，可以帮助重启它。

​		另外，使用`mysqld_safe`启动服务器程序时，它会将服务器程序的出错信息和其他诊断信息重定向到某个文件中，产生出错日志，这样可以方便我们找出发生错误的原因。

#### mysql.server

​		`mysql.server`也是一个启动脚本，它会间接的调用`mysqld_safe`，在调用`mysql.server`时在后边指定`start`参数就可以启动服务器程序了，就像这样：

```
mysql.server start
```

需要注意的是，这个 ***mysql.server*** 文件其实是一个链接文件，它的实际文件是 ***../support-files/mysql.server***。我使用的`macOS`操作系统会帮我们在`bin`目录下自动创建一个指向实际文件的链接文件，如果你的操作系统没有帮你自动创建这个链接文件，那就自己创建一个呗～ 别告诉我你不会创建链接文件，上网搜搜呗～

另外，我们还可以使用`mysql.server`命令来关闭正在运行的服务器程序，只要把`start`参数换成`stop`就好了：

```
mysql.server stop
```

#### mysqld_multi

​		其实我们一台计算机上也可以运行多个服务器实例，也就是运行多个`MySQL`服务器进程。`mysql_multi`可执行文件可以对每一个服务器进程的启动或停止进行监控

![image-20210318224053601](https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170637.png)

## 启动MySQL客户端程序

```
mysql -h主机名  -u用户名 -p密码
```

各个参数的意义如下：

| 参数名 | 含义                                                         |
| ------ | ------------------------------------------------------------ |
| `-h`   | 表示服务器进程所在计算机的域名或者IP地址，如果服务器进程就运行在本机的话，可以省略这个参数，或者填`localhost`或者`127.0.0.1`。也可以写作 `--host=主机名`的形式。 |
| `-u`   | 表示用户名。也可以写作 `--user=用户名`的形式。               |
| `-p`   | 表示密码。也可以写作 `--password=密码`的形式。               |

> 小贴士： 像 h、u、p 这样名称只有一个英文字母的参数称为短形式的参数，使用时前边需要加单短划线，像 host、user、password 这样大于一个英文字母的参数称为长形式的参数，使用时前边需要加双短划线。后边会详细讨论这些参数的使用方式的，稍安勿躁～

比如：

```mysql
mysql -hlocalhost -uroot -p123456
```

## 客户端与服务器连接的过程

​		运行着的服务器程序和客户端程序本质上都是计算机上的一个进程，所以客户端进程向服务器进程发送请求并得到回复的过程本质上是一个进程间通信的过程！`MySQL`支持下边三种客户端进程和服务器进程的通信方式。

### TCP/IP

​		生产环境中，数据库服务器进程和客户端进程可能运行在不同的主机中，它们之间必须通过网络来进行通讯。

​		`MySQL`采用`TCP`作为服务器和客户端之间的网络通信协议。通过`IP地址 + 端口号`的方式来与这个进程连接，这样进程之间就可以通过网络进行通信了。

​		`MySQL`服务器启动的时候会默认申请`3306`端口号，之后就在这个端口号上等待客户端进程进行连接【服务器会默认监听3306】

```
	如果`3306`端口号已经被别的进程占用了或者我们单纯的想自定义该数据库实例监听的端口号，那我们可以在启动服务器程序的命令行里添加`-P`参数来明确指定一下端口号，比如这样：mysqld -P3307，这样`MySQL`服务器在启动时就会去监听我们指定的端口号`3307`。
```

​		另外，如果服务器进程监听的端口号不是默认的`3306`，我们也可以在使用`mysql`启动客户端程序时使用`-P`参数（大写的`P`，小写的`p`是用来指定密码的）来指定需要连接到的端口号。

```
比如我们现在已经在本机启动了服务器进程，监听的端口号为`3307`，那我们启动客户端程序时可以这样写：:
mysql -h127.0.0.1 -uroot -P【大写指定端口】3307 -p【小写指定密码】
```

### 命名管道和共享内存（window）

​		如果你是一个`Windows`用户，那么客户端进程和服务器进程之间可以考虑使用`命名管道`或`共享内存`进行通信。不过启用这些通信方式的时候需要在启动服务器程序和客户端程序时添加一些参数：

- 使用`命名管道`来进行进程间通信

  需要在启动服务器程序的命令中加上`--enable-named-pipe`参数，然后在启动客户端程序的命令中加入`--pipe`或者`--protocol=pipe`参数。

- 使用`共享内存`来进行进程间通信

  需要在启动服务器程序的命令中加上`--shared-memory`参数，在成功启动服务器后，`共享内存`便成为本地客户端程序的默认连接方式，不过我们也可以在启动客户端程序的命令中加入`--protocol=memory`参数来显式的指定使用共享内存进行通信。

  不过需要注意的是，使用`共享内存`的方式进行通信的服务器进程和客户端进程必须在同一台`Windows`主机中。

> 小贴士： 命名管道和共享内存是Windows操作系统中的两种进程间通信方式，如果你没听过的话也不用纠结，并不妨碍我们介绍MySQL的知识～

### Unix域套接字文件

​		如果我们的服务器进程和客户端进程都运行在同一台操作系统为类`Unix`的机器上的话，我们可以使用`Unix域套接字文件`来进行进程间通信。如果我们在启动客户端程序的时候指定的主机名为`localhost`，或者指定了`--protocol=socket`的启动参数，那服务器程序和客户端程序之间就可以通过`Unix`域套接字文件来进行通信了。`MySQL`服务器程序默认监听的`Unix`域套接字文件路径为`/tmp/mysql.sock`，客户端程序也默认连接到这个`Unix`域套接字文件。如果我们想改变这个默认路径，可以在启动服务器程序时指定`socket`参数，就像这样：

```
mysqld --socket=/tmp/a.txt
```

​		这样服务器启动后便会监听`/tmp/a.txt`。在服务器改变了默认的`UNIX`域套接字文件后，如果客户端程序想通过`UNIX`域套接字文件进行通信的话，也需要显式的指定连接到的`UNIX`域套接字文件路径，就像这样：

```
mysql -hlocalhost -uroot --socket=/tmp/a.txt -p
```

这样该客户端进程和服务器进程就可以通过路径为`/tmp/a.txt`的`Unix`域套接字文件进行通信了。

## 服务器处理客户端请求

​		其实不论客户端进程和服务器进程是采用哪种方式进行通信，最后实现的效果都是：客户端进程向服务器进程发送一段文本（MySQL语句），服务器进程处理后再向客户端进程发送一段文本（处理结果）。那服务器进程对客户端进程发送的请求做了什么处理，才能产生最后的处理结果呢？客户端可以向服务器发送增删改查各类请求，我们这里以比较复杂的查询请求为例来画个图展示一下大致的过程：

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170638.png" alt="image-20210312155258288" style="zoom:50%;" />

从图中我们可以看出，服务器程序处理来自客户端的查询请求大致需要经过三个部分，分别是`连接管理`、`解析与优化`、`存储引擎`。下边我们来详细看一下这三个部分都干了什么。

### 连接管理

​		客户端进程可以采用我们上边介绍的`TCP/IP`、`命名管道或共享内存`、`Unix域套接字`这几种方式之一来与服务器进程建立连接，每当有一个客户端进程连接到服务器进程时，服务器进程都会创建一个线程来专门处理与这个客户端的交互，当该客户端退出时会与服务器断开连接，服务器并不会立即把与该客户端交互的线程销毁掉，而是把它缓存起来，在另一个新的客户端再进行连接时，把这个缓存的线程分配给该新客户端。这样就起到了不频繁创建和销毁线程的效果，从而节省开销。从这一点大家也能看出，`MySQL`服务器会为使用`线程连接池`为每一个连接进来的客户端分配一个线程，但是线程分配的太多了会严重影响系统性能，所以我们也需要限制一下可以同时连接到服务器的客户端数量，

​			在客户端程序发起连接的时候，需要携带主机信息、用户名、密码，服务器程序会对客户端程序提供的这些信息进行认证，如果认证失败，服务器程序会拒绝连接。另外，如果客户端程序和服务器程序不运行在一台计算机上，我们还可以采用使用了`SSL`（安全套接字）的网络连接进行通信，来保证数据传输的安全性。

​		当连接建立后，与该客户端关联的服务器线程会一直等待客户端发送过来的请求，`MySQL`服务器接收到的请求只是一个文本消息，该文本消息还要经过各种处理

### 解析与优化

到现在为止，`MySQL`服务器已经获得了文本形式的请求，接着 还要经过`查询缓存、语法解析、查询优化`

#### 查询缓存

​		如果我问你`9+8×16-3×2×17`的值是多少，你可能会用计算器去算一下，或者牛逼一点用心算，最终得到了结果`35`，如果我再问你一遍`9+8×16-3×2×17`的值是多少，你还用再傻呵呵的算一遍么？我们刚刚已经算过了，直接说答案就好了。

​		`MySQL`服务器程序处理查询请求的过程也是这样，会把刚刚处理过的查询请求和结果`缓存`起来，如果下一次有一模一样的请求过来，直接从缓存中查找结果就好了，就不用去底层的表中查找了。这个查询缓存可以在不同客户端之间共享，也就是说如果客户端A刚刚查询了一个语句，而客户端B之后发送了同样的查询请求，那么客户端B的这次查询就可以直接使用查询缓存中的数据。

​		当然，`MySQL`服务器并没有人聪明，如果两个查询请求在任何字符上的不同（例如：空格、注释、大小写），都会导致缓存不会命中。另外，如果查询请求中包含某些系统函数、用户自定义变量和函数、一些系统表，如 mysql 、information_schema、 performance_schema 数据库中的表，那这个请求就不会被缓存。以某些系统函数举例，可能同样的函数的两次调用会产生不一样的结果，比如函数`NOW`，每次调用都会产生最新的当前时间，如果在一个查询请求中调用了这个函数，那即使查询请求的文本信息都一样，那不同时间的两次查询也应该得到不同的结果，如果在第一次查询时就缓存了，那第二次查询的时候直接使用第一次查询的结果就是错误的！

​		不过既然是缓存，那就有它缓存失效的时候。MySQL的缓存系统会监测涉及到的每张表，只要该表的结构或者数据被修改，如对该表使用了`INSERT`、 `UPDATE`、`DELETE`、`TRUNCATE TABLE`、`ALTER TABLE`、`DROP TABLE`或 `DROP DATABASE`语句，那使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除！

> 小贴士： 虽然查询缓存有时可以提升系统性能，但也不得不因维护这块缓存而造成一些开销，比如每次都要去查询缓存中检索，查询请求处理完需要更新查询缓存，维护该查询缓存对应的内存区域。从MySQL 5.7.20开始，不推荐使用查询缓存，并在MySQL 8.0中删除。

#### 语法解析

如果查询缓存没有命中，接下来就需要进入正式的查询阶段了。因为客户端程序发送过来的请求只是一段文本而已，所以`MySQL`服务器程序首先要对这段文本做分析，判断请求的语法是否正确，然后从文本中将要查询的表、各种查询条件都提取出来放到`MySQL`服务器内部使用的一些数据结构上来。

> 小贴士： 这个从指定的文本中提取出我们需要的信息本质上算是一个编译过程，涉及词法解析、语法分析、语义分析等阶段，这些问题不属于我们讨论的范畴，大家只要了解在处理请求的过程中需要这个步骤就好了。

#### 查询优化

​		语法解析之后，服务器程序获得到了需要的信息，比如要查询的列是哪些，表是哪个，搜索条件是什么等等，但光有这些是不够的，因为我们写的`MySQL`语句执行起来效率可能并不是很高，`MySQL`的优化程序会对我们的语句做一些优化，如外连接转换为内连接、表达式简化、子查询转为连接吧啦吧啦的一堆东西。

​		优化的结果就是生成一个执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是啥样的。我们可以使用`EXPLAIN`语句来查看某个语句的执行计划，关于查询优化这部分的详细内容我们后边会仔细唠叨，现在你只需要知道在`MySQL`服务器程序处理请求的过程中有这么一个步骤就好了。

### 存储引擎

​		截止到服务器程序完成了查询优化为止，还没有真正的去访问真实的数据表，`MySQL`服务器把数据的存储和提取操作都封装到了一个叫`存储引擎`的模块里。我们知道`表`是由一行一行的记录组成的，但这只是一个逻辑上的概念，物理上如何表示记录，怎么从表中读取数据，怎么把数据写入具体的物理存储器上，这都是`存储引擎`负责的事情。为了实现不同的功能，`MySQL`提供了各式各样的`存储引擎`，不同`存储引擎`管理的表具体的存储结构可能不同，采用的存取算法也可能不同。

> 小贴士： 为什么叫`引擎`呢？因为这个名字更拉风～ 其实这个存储引擎以前叫做`表处理器`，后来可能人们觉得太土，就改成了`存储引擎`的叫法，它的功能就是接收上层传下来的指令，然后对表中的数据进行提取或写入操作。

​		为了管理方便，人们把`连接管理`、`查询缓存`、`语法解析`、`查询优化`这些并不涉及真实数据存储的功能划分为`MySQL server`的功能，把真实存取数据的功能划分为`存储引擎`的功能。各种不同的存储引擎向上边的`MySQL server`层提供统一的调用接口（也就是存储引擎API），包含了几十个底层函数，像"读取索引第一条内容"、"读取索引下一条内容"、"插入记录"等等。

​		所以在`MySQL server`完成了查询优化后，只需按照生成的执行计划调用底层存储引擎提供的API，获取到数据后返回给客户端。



## 常用存储引擎

`MySQL`支持非常多种存储引擎，我这先列举一些：

| 存储引擎    | 描述                                 |
| ----------- | ------------------------------------ |
| `ARCHIVE`   | 用于数据存档（行被插入后不能再修改） |
| `BLACKHOLE` | 丢弃写操作，读操作会返回空内容       |
| `CSV`       | 在存储数据时，以逗号分隔各个数据项   |
| `FEDERATED` | 用来访问远程表                       |
| `InnoDB`    | 具备外键支持功能的事务存储引擎       |
| `MEMORY`    | 置于内存的表                         |
| `MERGE`     | 用来管理多个MyISAM表构成的表集合     |
| `MyISAM`    | 主要的非事务处理存储引擎             |
| `NDB`       | MySQL集群专用存储引擎                |

## 关于存储引擎的一些操作

### 查看当前服务器程序支持的存储引擎

我们可以用下边这个命令来查看当前服务器程序支持的存储引擎：

```
SHOW ENGINES;
```

来看一下调用效果：

```
mysql> SHOW ENGINES;
+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+
| Engine             | Support | Comment                                                        | Transactions | XA   | Savepoints |
+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+
| InnoDB             | DEFAULT | Supports transactions, row-level locking, and foreign keys     | YES          | YES  | YES        |
| MRG_MYISAM         | YES     | Collection of identical MyISAM tables                          | NO           | NO   | NO         |
| MEMORY             | YES     | Hash based, stored in memory, useful for temporary tables      | NO           | NO   | NO         |
| BLACKHOLE          | YES     | /dev/null storage engine (anything you write to it disappears) | NO           | NO   | NO         |
| MyISAM             | YES     | MyISAM storage engine                                          | NO           | NO   | NO         |
| CSV                | YES     | CSV storage engine                                             | NO           | NO   | NO         |
| ARCHIVE            | YES     | Archive storage engine                                         | NO           | NO   | NO         |
| PERFORMANCE_SCHEMA | YES     | Performance Schema                                             | NO           | NO   | NO         |
| FEDERATED          | NO      | Federated MySQL storage engine                                 | NULL         | NULL | NULL       |
+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+
9 rows in set (0.00 sec)

mysql>
```

​			其中的`Support`列表示该存储引擎是否可用，`DEFAULT`值代表是当前服务器程序的默认存储引擎。`Comment`列是对存储引擎的一个描述，英文的，将就着看吧。`Transactions`列代表该存储引擎是否支持事务处理。`XA`列代表着该存储引擎是否支持分布式事务。`Savepoints`代表着该存储引擎是否支持部分事务回滚。

### 设置表的存储引擎

我们前边说过，存储引擎是负责对表中的数据进行提取和写入工作的，我们可以为不同的表设置不同的存储引擎，也就是说不同的表可以有不同的物理存储结构，不同的提取和写入方式。

#### 创建表时指定存储引擎

我们之前创建表的语句都没有指定表的存储引擎，那就会使用默认的存储引擎`InnoDB`（当然这个默认的存储引擎也是可以修改的，我们在后边的章节中再说怎么改）。如果我们想显式的指定一下表的存储引擎，那可以这么写：

```sql
CREATE TABLE 表名(
    建表语句;
) ENGINE = 存储引擎名称;
```

比如我们想创建一个存储引擎为`MyISAM`的表可以这么写：

```sql
 CREATE TABLE engine_demo_table(
    i int
) ENGINE = MyISAM;

```

#### 修改表的存储引擎

如果表已经建好了，我们也可以使用下边这个语句来修改表的存储引擎：

```sql
ALTER TABLE 表名 ENGINE = 存储引擎名称;
```

比如我们修改一下`engine_demo_table`表的存储引擎：

```sql
mysql> ALTER TABLE engine_demo_table ENGINE = InnoDB;
Query OK, 0 rows affected (0.05 sec)
Records: 0  Duplicates: 0  Warnings: 0
```

这时我们再查看一下`engine_demo_table`的表结构：

```sql
mysql> SHOW CREATE TABLE engine_demo_table\G
*************************** 1. row ***************************
       Table: engine_demo_table
Create Table: CREATE TABLE `engine_demo_table` (
  `i` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8
1 row in set (0.01 sec)
```

可以看到该表的存储引擎已经改为`InnoDB`了。



## MyISAM与innodb区别是什么

1. **是否支持行级锁** : MyISAM 只有表级锁，而InnoDB 支持行级锁和表级锁,默认行级锁。
2. **是否支持事务和崩溃后的安全恢复： MyISAM** 强调的是性能，每次查询具有原子性,其执行速度比InnoDB类型更快，但是不提供事务支持。但是**InnoDB** 提供事务支持，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全型表。
3. **是否支持外键：** MyISAM不支持，而InnoDB支持。
4. **是否支持MVCC** ：仅 InnoDB 支持。应对高并发事务, MVCC比单纯的加锁更高效;MVCC只在 `READ COMMITTED` 和 `REPEATABLE READ` 两个隔离级别下工作;MVCC可以使用 乐观(optimistic)锁 和 悲观(pessimistic)锁来实现;各数据库中MVCC实现并不统一。推荐阅读：[MySQL-InnoDB-MVCC多版本并发控制](https://segmentfault.com/a/1190000012650596)

### MyISAM适用场景

1. 频发执行全表count语句：MyISAM会用一个变量存储表的行数，而innodb不会存储
2. 对数据增删改的频率不高，查询非常频繁
3. 不支持事务，不在乎崩溃后安全恢复

### innodb适用场景

1. 增删改查都频繁
2. 要求支持事务，要求崩溃后安全恢复



## 字符集

### MySQL中的utf8和utf8mb4

我们上边说`utf8`字符集表示一个字符需要使用1～4个字节，但是我们常用的一些字符使用1～3个字节就可以表示了。而在`MySQL`中字符集表示一个字符所用最大字节长度在某些方面会影响系统的存储和性能，所以设计`MySQL`的大叔偷偷的定义了两个概念：

- `utf8mb3`：阉割过的`utf8`字符集，只使用1～3个字节表示字符。
- `utf8mb4`：正宗的`utf8`字符集，使用1～4个字节表示字符。

有一点需要大家十分的注意，在`MySQL`中`utf8`是`utf8mb3`的别名，所以之后在`MySQL`中提到`utf8`就意味着使用1~3个字节来表示一个字符，如果大家有使用4字节编码一个字符的情况，比如存储一些emoji表情啥的，那请使用`utf8mb4`。

# 索引

> 为什么要使用索引？：避免全表扫描，提升查询效率
>
> 什么样的信息可以成为索引？：主键、唯一键，只要是区分度较大的，就可以称为索引
>
> 索引的数据结构：主流是B+树索引结构
>
> 密集索引和稀疏索引的区别

<span style='color:red;background:white;font-size:30;font-family:微软雅黑;'>文字</span> 



## B-Tree(B树)

### 定义

1. 根节点至少包括两个孩子
2. 树中每个节点最多含有m个孩子（m>=2）【m取决于容量及相关配置】
3. 除根节点和叶子节点外，其他每个节点至少有ceil(m(/2))个孩子【ceil表示小数点，取上限，比如m/2=1.1,则ceil(1.1)=2】
4. 所有叶子节点都位于同一层
5. 非叶子节点也可以存储数据【和B+的区别】

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171559.png" alt="image-20210228224051057" style="zoom: 33%;" />

## B+-Tree(B+树)

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171600.png" alt="image-20210228225205223" style="zoom: 33%;" />



B+树是B树的变体，其定义基本与B树相同，除了：

1. 非叶子结点的子树指针与关键字个数相同（B树只有两个关键字，表明B+树可以存储更多的关键字）

   ![image-20210228225402467](https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171601.png)

2. 非叶子结点的子树指针P[i]，指向关键字值[ K[i] , K[i+1] )的子树【[ K[i] , K[i+1] )，这个表达式的意思是前闭后开】

   ![image-20210228230108925](https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171602.png)

   设p[i]为p2，k[i]=10，k[i+1]=20

   则：p2指向的子树的值x1...xn,必须满足： 10**<=**x1...xn**<**20

3. 非叶子节点仅用来做索引，不保存数据，数据都保存在叶子节点中

4. 所有叶子节点均有一个链指针指向下一个叶子节点，并且按大小顺序链接 

   ![image-20210228230543081](https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171603.png)

   ​	[5,8,9]  -> [10,15,18] -> [20,26,27] -> [28,30,33] -> [35,38,50]

### B+树比B树更加适合做存储索引

1. B+数的磁盘读写代价更低

   > b树非叶子节点只存储索引关键字信息，不存储数据信息，因此非叶子结点相比于b树更小，按照索引查找的话，相对于同一块磁盘块，B+树可以读取更多的索引信息，减少IO次数

2. B+树的查询效率更加稳定

   > 因为B+树的数据是存放在叶子节点的，所以查询时，一定是按照**从根节点到叶子节点**查的，查询效率稳定O(logn)；而B树的非叶子结点也可以存储数据，可能在非叶子结点就查到数据并返回了，且B树检索的过程是按照二分查找的，查询效率不稳定

3. B+树更有利于对数据库的扫描

   > B+的叶子节点是链接起来的，当做范围查找的时候B+树的优势远远大于B树
   
4. B+树适合作为数据库的基础结构，完全是因为计算机的内存-机械硬盘两层存储结构

   ​			内存可以完成快速的随机访问（随机访问即给出任意一个地址，要求返回这个地址存储的数据）但是容量较小。而硬盘的随机访问要经过机械动作（1磁头移动 2盘片转动），访问效率比内存低几个数量级，但是硬盘容量较大。典型的数据库容量大大超过可用内存大小，这就决定了在B+树中检索一条数据很可能要借助几次磁盘IO操作来完成。如下图所示：通常向下读取一个节点的动作可能会是一次磁盘IO操作，不过非叶节点通常会在初始阶段载入内存以加快访问速度。同时为提高在节点间横向遍历速度，真实数据库中可能会将图中蓝色的CPU计算/内存读取优化成二叉搜索树（InnoDB中的page directory机制）。

   <img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170639.png" alt="image-20210315230003849" style="zoom:50%;" />

   ​		真实数据库中的B+树应该是非常扁平的，可以通过向表中顺序插入足够数据的方式来验证InnoDB中的B+树到底有多扁平。我们通过如下图的CREATE语句建立一个只有简单字段的测试表，然后不断添加数据来填充这个表。通过下图的统计数据（来源见参考文献1）可以分析出几个直观的结论，这几个结论宏观的展现了数据库里B+树的尺度。

　　1 每个叶子节点存储了468行数据，每个非叶子节点存储了大约1200个键值，这是一棵平衡的1200路搜索树！

　　2 对于一个22.1G容量的表，也只需要高度为3的B+树就能存储了，这个容量大概能满足很多应用的需要了。如果把高度增大到4，则B+树的存储容量立刻增大到25.9T之巨！

　　3 对于一个22.1G容量的表，B+树的高度是3，如果要把非叶节点全部加载到内存也只需要少于18.8M的内存（如何得出的这个结论？因为对于高度为2的树，1203个叶子节点也只需要18.8M空间，而22.1G从良表的高度是3，非叶节点1204个。同时我们假设叶子节点的尺寸是大于非叶节点的，因为叶子节点存储了行数据而非叶节点只有键和少量数据。），只使用如此少的内存就可以保证只需要一次磁盘IO操作就检索出所需的数据，效率是非常之高的。

![image-20210315230026539](https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170640.png)



### MyIsam和innodb对B+树的实现不同

#### MyISAM索引的实现

**MyISAM索引文件和数据文件是分离的**，叶子节点仅仅存储数据行所在的页的地址，通过地址再回表查询页中的数据行

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171604.png" alt="image-20210303171459635" style="zoom:50%;" />

上图所提供的示例表字段有Col1（ID）、Col2(age)、Col3（name）三个，其中Col1为Primary Key（主键），上图很好地说明了树中叶子保存的是对应行的物理位置。通过该值，存储引擎能顺利地进行回表查询【通过地址再查页】，得到一行完整记录。同时，每个叶子页也保存了指向下一个叶子页的指针。从而方便叶子节点的范围遍历。
而对于二级索引，在 MyISAM存储引擎中以与上图同样的方式实现，这也说明了 MyISAM的索引方式是“非聚集的”，与 Innodb的“聚集索引”形成了对比。

#### InnoDB索引的实现

##### 聚集索引

InnoDB 存储引擎采用**“聚集索引”**的数据存储方式实现B+-Tree索引，所谓“聚集”，就是指数据行和相邻的键值紧凑地存储在一起，注意 InnoDB 只能聚集一个叶子页（16K）的记录（即聚集索引满足一定的范围的记录），因此包含相邻键值的记录可能会相距甚远。

在 InnoDB 中，表被称为 索引组织表（index organized table），InnoDB 按照主键构造一颗 B+Tree （如果没有主键，则会选择一个唯一的并且非空索引替代，如果没有这样的索引，InnoDB则会隐式地定义一个主键来作为聚集索引），同时叶子页中存放整张表的行记录数据【注意区别于myisam】，也可以将聚集索引的叶子节点称为数据页，非叶子页可以看做是叶子页的稀疏索引。

下图说明了 InnoDB聚集索引的实现方式，同时也体现了一张 innoDB表的结构，可以看到，InnoDB 中，主键索引和数据是一体的，没有分开。：

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171605.png" alt="image-20210303171533733" style="zoom:50%;" />

这种实现方式，给予了 InnoDB 按主键检索的超高性能。可以有目的性地选择聚集索引，比如一个邮件表，可以选择用户ID来聚集数据，这样只需要从磁盘读取较少并且连续的数据页就能获得某个id的用户全部的邮件，避免了读取分散页时所耗费的随机I/O。

##### 辅助索引

而对于辅助索引，InnoDB采用的方式是在叶子页中保存主键值，通过这个主键值来回表（上图）查询到一条完整记录，因此按辅助索引检索实际上进行了二次查询，效率肯定是没有按照主键检索高的。下图是辅助索引的实现方式：

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171606.png" alt="image-20210303171548369" style="zoom:50%;" />

由于每个辅助索引都包含主键索引，因此，为了减小辅助索引所占空间，我们通常希望 InnoDB 表中的主键索引尽量定义得小一些（值得一提的是，MySIAM会使用前缀压缩技术使得索引变小，而InnoDB按照原数据格式进行存储。），并且希望InnoDB的主键是自增长的，因为如果主键并非自增长，插入时，由于写入时乱序的，会使得插入效率变低。

## Hash索引

对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。

缺点：

1. 其数据结构就决定了，hash索引 仅支持=或者in，无法支持**范围查找**
2. 无法被用来避免数据的排序操作：因为hash存储的值是乱序的
3. 不能利用部分索引键查询：对于hash组合索引，where条件必须有所有的索引，hash索引才会有效
4. 遇到大量Hash值相等的情况后，其性能并不一定就会比B树索引高【hash冲突】
5. **Hash 索引不支持顺序和范围查询(Hash 索引不支持顺序和范围查询是它最大的缺点**

## BitMap(位图索引)

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171607.png" alt="image-20210301202704653" style="zoom:70%;" />

1. 目前只有oracle支持
2. 只适用于字段只有固定的几个值



## 索引类型

### 主键索引

数据表的主键列使用的就是主键索引，一张数据表有只能有一个主键，并且主键不能为 null，不能重复

在 mysql 的 InnoDB 的表中，确定主键：如果有显示定义的主键，则就是该主键；如果没有显示定义的主键则找唯一索引，确定为主键；如果没有唯一索引，**则 InnoDB 将会自动创建一个 6Byte 的自增主键**

### 二级索引(辅助索引)

**二级索引又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置。**

唯一索引，普通索引，前缀索引等索引属于二级索引。

**PS:不懂的同学可以暂存疑，慢慢往下看，后面会有答案的，也可以自行搜索。**

1. **唯一索引(Unique Key)** ：唯一索引也是一种约束。**唯一索引的属性列不能出现重复的数据，但是允许数据为 NULL，一张表允许创建多个唯一索引。** 建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。
2. **普通索引(Index)** ：**普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和 NULL。**
3. **前缀索引(Prefix)** ：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。
4. **全文索引(Full Text)** ：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6 之前只有 MYISAM 引擎支持全文索引，5.6 之后 InnoDB 也支持了全文索引。

二级索引:

![image-20210307124407548](https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210307125322.png)

通过叶子结点的主键，回表查询主键的索引树，最终找到数据行

## 聚集索引和非聚集索引

###  聚集索引

**聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。**

在 Mysql 中，InnoDB 引擎的表的 `.ibd`文件就包含了该表的索引和数据，对于 InnoDB 引擎表来说，该表的索引(B+树)的每个非叶子节点存储索引，叶子节点存储索引和索引对应的数据。

#### [聚集索引的优点](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=聚集索引的优点)

聚集索引的查询速度非常的快，因为整个 B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。

#### [聚集索引的缺点](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=聚集索引的缺点)

1. **依赖于有序的数据** ：因为 B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢。
2. **更新代价大** ： 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。

### 非聚集索引

**非聚集索引即索引结构和数据分开存放的索引。**

**二级索引属于非聚集索引。**

> MYISAM 引擎的表的.MYI 文件包含了表的索引， 该表的索引(B+树)的每个叶子非叶子节点存储索引， 叶子节点存储索引和索引对应数据的指针，指向.MYD 文件的数据。
>
> **非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。**

#### [非聚集索引的优点](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=非聚集索引的优点)

**更新代价比聚集索引要小** 。非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的

#### [非聚集索引的缺点](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=非聚集索引的缺点)

1. 跟聚集索引一样，非聚集索引也依赖于有序的数据
2. **可能会二次查询(回表)** :这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。

这是 Mysql 的表的文件截图:

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210307125323.png" alt="image-20210307124622936" style="zoom:50%;" />

聚集索引和非聚集索引:

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210307125324.png" alt="image-20210307124651209" style="zoom:50%;" />





## 密集索引和稀疏索引

密集索引：密集索引文件中的每个搜索码值都对应一个索引值，叶子节点保存的不只是键值，还保存了位于同一行记录里的其他列的信息，由于密集索引决定了表的物理排列顺序，一个表只有一个物理排列顺序，所以一个表只能创建一个密集索引

稀疏索引：稀疏索引文件只为索引码的某些值建立索引项

### mysam和innodb

mysam存储引擎，不管是主键索引，唯一键索引还是普通索引都是稀疏索引

innodb存储引擎：**有且只有一个密集索引**。密集索引的选取规则如下：

- 若主键被定义，则主键作为密集索引
- 如果没有主键被定义，该表的第一个唯一非空索引则作为密集索引
- 若不满足以上条件，innodb内部会生成一个隐藏主键（密集索引）
- 非主键索引存储相关键位和其对应的主键值，包含两次查找（回表）

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171608.png" alt="image-20210228233946334" style="zoom:50%;" />

​		我们重点关注聚簇索引，**看上去聚簇索引的效率明显要低于非聚簇索引**，因为每次使用辅助索引检索都要经过两次B+树查找，这不是多此一举吗？聚簇索引的优势在哪？

　　1 由于行数据和叶子节点存储在一起，这样主键和行数据是**一起**被载入内存的，找到叶子节点就可以**立刻**将行数据返回了，如果按照主键Id来组织数据，获得数据更快。

　　2 辅助索引**使用主键作为"指针"** 而不是使用地址值作为指针的好处是，减少了当出现行移动或者数据页分裂时辅助索引的维护工作，使用主键值当作指针会让辅助索引占用更多的空间，换来的好处是InnoDB在移动行时无须更新辅助索引中的这个"指针"。也就是说行的位置（实现中通过16K的Page来定位，后面会涉及）会随着数据库里数据的修改而发生变化（前面的B+树节点分裂以及Page的分裂），使用聚簇索引就可以保证不管这个主键B+树的节点如何变化，辅助索引树都不受影响。

### Page结构

　　如果说前面的内容偏向于解释原理，那后面就开始涉及具体实现了。

　　理解InnoDB的实现不得不提Page结构，Page是整个InnoDB存储的最基本构件，也是InnoDB磁盘管理的最小单位【16K】，与数据库相关的所有内容都存储在这种Page结构里。

​		Page分为几种类型，常见的页类型有数据页（B-tree Node）Undo页（Undo Log Page）系统页（System Page） 事务数据页（Transaction System Page）等。单个Page的大小是16K（编译宏UNIV_PAGE_SIZE控制），每个Page使用一个32位的int值来唯一标识，这也正好对应InnoDB最大64TB的存储容量（16Kib * 2^32 = 64Tib）。一个Page的基本结构如下图所示：

​							<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170641.png" alt="image-20210315222522799" style="zoom:70%;" />

#### page头部　　

每个Page都有通用的头和尾，但是中部的内容根据Page的类型不同而发生变化。Page的头部里有我们关心的一些数据，下图把Page的头部详细信息显示出来：

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170642.png" alt="image-20210315230149910" style="zoom:50%;" />





![image-20210315222627939](https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170643.png)

​		　　我们重点关注和数据组织结构相关的字段：Page的头部保存了两个指针，分别指向前一个Page和后一个Page，头部还有Page的类型信息和用来唯一标识Page的编号。根据这两个指针我们很容易想象出Page链接起来就是一个双向链表的结构。

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170644.png" alt="image-20210315222742068" style="zoom:50%;" />

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210315230235.png" alt="image-20210315230233763" style="zoom:50%;" />



#### page主体内容

　　再看看Page的主体内容，我们主要关注行数据和索引的存储，他们都位于Page的User Records部分，User Records占据Page的大部分空间，User Records由一条一条的Record组成，每条记录代表索引树上的一个节点（非叶子节点和叶子节点）。

​		在一个Page内部，单链表的头尾由固定内容的两条记录来表示，字符串形式的"Infimum【下限】"代表开头，"Supremum【上限】"代表结尾。这两个用来代表开头结尾的Record存储在System Records的段里，这个System Records和User Records是两个平行的段。

​		InnoDB存在4种不同的Record，它们分别是：

   1. 主键索引树非叶节点 

   2. 主键索引树叶子节点 

   3. 辅助键索引树非叶节点 

   4. 辅助键索引树叶子节点。

      ​		这4种节点的Record格式有一些差异，但是它们都存储着**Nex**t指针指向下一个Record。后续我们会详细介绍这4种节点，现在只需要把Record【数据+指针的单链表节点】当成一个存储了数据同时含有Next指针的单链表节点即可。

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170645.png" alt="image-20210315223211177" style="zoom:50%;" />

　　User Record在Page内以单链表的形式存在，最初数据是按照插入的先后顺序排列的，但是随着新数据的插入和旧数据的删除，数据物理顺序会变得混乱，但他们依然保持着逻辑上的先后顺序。

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170646.png" alt="image-20210315223251691" style="zoom:50%;" />

　　把User Record的组织形式和若干Page组合起来，就看到了稍微完整的形式。

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170647.png" alt="image-20210315223308657" style="zoom:50%;" />



　　现在看下如何定位一个Record：

1. 通过根节点开始遍历一个索引的B+树，通过各层非叶子节点最终到达一个Page，这个Page里存放的都是叶子节点。

2. 在Page内从"Infimum"节点开始遍历单链表（这种遍历往往会被优化），如果找到该键则成功返回。如果记录到达了"supremum"，说明当前Page里没有合适的键，这时要借助Page的Next Page指针，跳转到下一个Page继续从"Infimum"开始逐个查找

   【page1->infimum->supremum】->【page2->infimum->supremum】->【page3->infimum->supremum】

   <img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170648.png" alt="image-20210315223948864" style="zoom:50%;" />



​		详细看下不同类型的Record里到底存储了什么数据，根据B+树节点的不同，User Record可以被分成四种格式，下图种按照颜色予以区分。

**1 主索引树非叶节点（绿色）**

　　1. 子节点存储的主键里最小的值（Min Cluster Key on Child 叶子节点链表中的最小值），这是B+树必须的，作用是在一个Page里定位到具体的记录的位置。

　　2. 最小的值所在的Page的编号（Child Page Number），作用是定位Record。

**2 主索引树叶子节点（黄色）**

　　1 主键（Cluster Key Fields），B+树必须的，也是数据行的一部分

　　2 除去主键以外的所有列（Non-Key Fields），这是数据行的除去主键的其他所有列的集合。

　　这里的1和2两部分加起来就是一个完整的数据行。

**3 辅助索引树非叶节点非（蓝色）**

　　1 子节点里存储的辅助键值里的最小的值（Min Secondary-Key on Child），这是B+树必须的，作用是在一个Page里定位到具体的记录的位置。

　　2 主键值（Cluster Key Fields），非叶子节点为什么要存储主键呢？因为辅助索引是可以不唯一的，但是B+树要求键的值必须唯一，所以这里把辅助键的值和主键的值合并起来作为在B+树中的真正键值，保证了唯一性。但是这也导致在辅助索引B+树中非叶节点反而比叶子节点多了4个字节。（即下图中蓝色节点反而比红色多了4字节）

　　3 最小的值所在的Page的编号（Child Page Number），作用是定位Record。

**4 辅助索引树叶子节点（红色）**

　　1 辅助索引键值（Secondary Key Fields），这是B+树必须的。

　　2 主键值（Cluster Key Fields），用来在主索引树里再做一次B+树检索来找到整条记录。

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170649.png" alt="image-20210315224046156" style="zoom:50%;" />

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170650.png" alt="image-20210315225649939" style="zoom:50%;" />





　　下面是本篇最重要的部分了，结合B+树的结构和前面介绍的4种Record的内容，我们终于可以画出一幅全景图。由于辅助索引的B+树与主键索引有相似的结构，这里只画出了主键索引树的结构图，只包含了"主键非叶节点"和"主键叶子节点"两种节点，也就是上图的的绿色和黄色的部分。

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170651.png" alt="image-20210315225022569" style="zoom:50%;" />

　　把上图还原成下面这个更简洁的树形示意图，这就是B+树的一部分。注意Page和B+树节点之间并没有一一对应的关系，Page只是作为一个Record的保存容器，它存在的目的是便于对磁盘空间进行批量管理，上图中的编号为47的Page在树形结构上就被拆分成了两个独立节点。

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210319170652.png" alt="image-20210315225044094" style="zoom:50%;" />

## 覆盖索引

### 什么是覆盖索引

如果一个索引包含（或者说覆盖）所有需要select的字段，我们就称之为“覆盖索引”。我们知道InnoDB存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！

### 例子

现在我创建了索引(username,age)，我们执行下面的 sql 语句

```sql
select username , age from user where username = 'Java' and age = 22
```

在查询数据的时候：要查询出的列在叶子节点都存在！所以，就不用回表。

## 调优SQL

> 如何定位并优化慢查询sql
>
> 联合索引的最左匹配原则的成因
>
> 索引是建立的越多越好吗

### 编写利用索引和选择索引的查询语句的三个原则

1. 单行访问是很慢的。特别是在机械硬盘存储中(SSD的随机I/O要快很多，不过这一点仍然成立）。如果服务器从存储中读取一个数据块只是为了获取其中一行，那么就浪费了很多工作。最好读取的块中能包含尽可能多所需要的行。使用索引可以创建位置引，用以提升效率。
2. 按顺序访问范围数据是很快的，这有两个原因。第一，顺序 I/O 不需要多次磁盘寻道，所以比随机I/O要快很多（特别是对机械硬盘）。第二，如果服务器能够按需要顺序读取数据，那么就不再需要额外的排序操作，并且GROUPBY查询也无须再做排序和将行按组进行聚合计算了。
3. 索引覆盖查询是很快的。如果一个索引包含了查询需要的所有列，那么存储引擎就 不需要再回表查找行。这避免了大量的单行访问，而上面的第1点已经写明单行访 问是很慢的。

  ### 如何定位并优化慢查询sql

1. 根据慢日志定位慢查询sql

   1.1：数据库慢查询配置：show variables like '%query%';

   <img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171609.png" alt="image-20210301204318480" style="zoom:50%;" />

   开启“slow_query_log=on”慢查询记录，当sql执行时间大于5s，则将该条慢查询sql记录在“slow_query_log_file=/data/mysql3306/log/mariadb-slow.log”【这个文件中存的是慢查询的sql】

    1.2：当前mysql的慢查询数量：show variables like '%slow_queries%';

2. 使用explain等工具分析sql

   

   <img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171610.png" alt="image-20210301205603662" style="zoom:50%;" />

   expalin各个字段的含义？

   id：表示查询中执行select子句或者操作表的顺序，**`id`的值越大，代表优先级越高，越先执行**

   table：查询的表名，并不一定是真实存在的表，有别名显示别名，也可能为临时表

   type：system(最快) > const > eq_ref > ref > range > index > ALL(最慢)，如果出现ALL，则说明该条sql需要优化了

   | type名 | 含义                                                         |
   | ------ | :----------------------------------------------------------- |
   | system | 当表仅有一行记录时(系统表)，数据量很少，往往不需要进行磁盘IO，速度非常 |
   | const  | `const`是直接按主键或唯一键读取，最多有一个匹配行            |
   | eq_ref | `eq_ref`用于`联表`查询的情况，按联表的主键或唯一键联合查询   |
   | ref    | `ref`表示使用非唯一性索引，会找到很多个符合条件的行。        |
   | range  | 使用索引选择行，仅检索给定范围内的行。简单点说就是针对一个有索引的字段，给定范围检索数据。在`where`语句中使用 `bettween...and`、`<`、`>`、`<=`、`in` 等条件查询 `type` 都是 `range`。 |
   | index  | `Index` 与`ALL` 其实都是读全表，区别在于`index`是遍历索引树读取，而`ALL`是从硬盘中读取 |
   | ALL    | 将遍历全表以找到匹配的行，性能最差                           |

   Extra：

   1、Using index

   `Using index`：我们在相应的 `select` 操作中使用了覆盖索引，通俗一点讲就是查询的列被索引覆盖，使用到覆盖索引查询速度会非常快，`SQl`优化中理想的状态。

   2、Using temporary

   `Using temporary`：表示查询后结果需要使用临时表来存储，一般在排序或者分组查询时用到。

   3、 

   <img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171611.png" alt="image-20210301210328724" style="zoom:50%;" />

3. 修改sql或者尽量让sql走索引

### 联合索引的最左匹配原则的成因

#### 最左匹配原则

![image-20210301212342748](https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171612.png) 

注意：<,>,like,between,不包括in

#### 成因

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171613.png" alt="image-20210301212840215" style="zoom:50%;" />

​		如图：创建组合索引 (col3,col2,col1),则会创建如右图中的数据结构，先匹配col3的数据，然后再匹配col2和col1的数据，所以一定where条件中有col3才会走组合索引；这就是最左匹配原则的成因

### 索引是创建的越多越好吗？

索引的缺点：

1. **创建索引和维护索引需要耗费许多时间**：当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执行效率。
2. **占用物理存储空间** ：索引需要使用物理文件存储，也会耗费一定空间。

## Other Q

### 为什么索引能提高查询速度

>  以下内容整理自： 地址： https://juejin.im/post/5b55b842f265da0f9e589e79 作者 ：Java3y

#### 先从MySql的基础存储结构说起

MySQL的基本存储结构是页(记录都存在页里边)：

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210304154254.png" alt="image-20210304153210616" style="zoom:50%;" />

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210304154255.png" alt="image-20210304153240590" style="zoom:50%;" />

- **各个数据页可以组成一个双向链表**

- 每个数据页中的记录又可以组成一个单向链表

  ```markup
  - 每个数据页都会为存储在它里的记录生成一个页目录【Page Directory】，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录
  - 以其他列(非主键)作为搜索条件：只能从最小记录开始依次遍历单链表中的每条记录。
  ```

所以说，如果我们写select * from user where indexname = 'xxx'这样没有进行任何优化的sql语句，默认会这样做：

1. **定位到记录所在的页：需要遍历双向链表，找到所在的页**
2. **从所在的页内中查找相应的记录：由于不是根据主键查询，只能遍历所在页的单链表了**

很明显，在数据量很大的情况下这样查找会很慢！这样的时间复杂度为O（n）。

##### 使用了索引之后

索引做了些什么可以让我们查询加快速度呢？其实就是将无序的数据变成有序(相对)：



<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210304154256.png" alt="image-20210304153629094" style="zoom:50%;" />

要找到id为8的记录简要步骤：

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210304154257.png" alt="image-20210304153705822" style="zoom:50%;" />

很明显的是：没有用索引我们是需要遍历双向链表来定位对应的页，现在通过 **“目录”** 就可以很快地定位到对应的页上了！（二分查找，时间复杂度近似为O(logn)）

其实底层结构就是B+树，B+树作为树的一种实现，能够让我们很快地查找出对应的记录。

# 锁

**Mysql为了解决并发、数据安全的问题，使用了锁机制。**

## InnoDB

**InnoDB存储引擎的锁的算法有三种：**

- Record lock：行级锁record lock
- Gap lock：间隙锁，锁定一个范围，不包括记录本身
- Next-key lock：record lock+gap lock 锁定一个范围，包含记录本身

**相关知识点：**

1. Next-key lock可以解决幻读问题

2. innodb对于行的查询使用next-key lock；当查询的索引含有唯一属性时，将next-key lock降级为record key

3. Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生

4. 有两种方式显式关闭gap锁：

   ​	 A. 将事务隔离级别设置为RC 【Read Commit，当大于rc的时候才可以使用gap锁】

   ​	B. 将参数innodb_locks_unsafe_for_binlog设置为1

## 锁的分类

### 粒度划分(表级锁、行级锁)

#### 表级锁

1. 对当前操作的整张表加锁，粒度最大的一种锁，不会出现死锁，
3. 并发度最低
4. MyISAM和 InnoDB引擎都支持表级锁。

#### 行级锁

​		Mysql中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。 InnoDB支持的行级锁，包括如下几种：

* Record Lock: 对索引项加锁，锁定符合条件的行。其他事务不能修改和删除加锁项；
* Gap Lock: 对索引项之间的“间隙”加锁，锁定记录的范围（对第一条记录前的间隙或最后一条将记录后的间隙加锁），不包含索引项本身。其他事务不能在锁范围内**插入数据**，这样就防止了别的事务 新增幻影行。
* Next-key Lock： 锁定索引项本身和索引范围。即Record Lock和Gap Lock的结合。可解决幻读问题。
  虽然使用行级索具有粒度小、并发度高等特点，但是表级锁有时候也是非常必要的：

事务更新大表中的大部分数据直接使用表级锁效率更高；
事务比较复杂，使用行级索很可能引起死锁导致回滚。

### 锁分类（按照是否可写分类）

#### 共享锁（s）

​		共享锁（Share Locks，简记为S）又被称为读锁

1. 本事务可以对加锁数据进行增删改查
2. 其他事务**只能**加共享锁进行读，**不能**获取数据上的**排他锁**，直到本事务提交释放共享锁。

添加共享锁的方式：``select * from pop_settlement.test1 where id=1 lock in share mode ;``

#### 排他锁（X）：

​		排它锁（(Exclusive lock,简记为X锁)）又称为写锁，

1. 本事务对数据对象A加上X锁，则只允许本事务对A进行read、update、delete
2. 其它任何事务都不能再对A加**任何**类型的锁，直到本事务释放A上的锁。
3. 在更新操作(INSERT、UPDATE 或 DELETE)时，mysql会自动加上排它锁，无需手动添加。

#### 扩展知识

对于innoDB，当update的时候，where条件没有索引列，则innoDB会将整张表加表锁

对于innoDB，当update的时候，where条件有索引列，则innoDB只会将索引查询出来的相关的数据加行级锁或者间隙锁

### 乐观锁和悲观锁

> 参考：https://www.jianshu.com/p/d2ac26ca6525
>
> 乐观锁比较适用于读多写少的情况(多读场景)
>
> 悲观锁比较适用于写多读少的情况(多写场景)

#### 乐观锁

##### 定义

乐观锁**假设**数据一般情况下**不会造成冲突**，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则返回给用户错误的信息，让用户决定如何去做。乐观锁适用于读操作多的场景，这样可以提高程序的吞吐量。

##### 实现

1. CAS 思想应用：Java 中java.util.concurrent.atomic【https://www.jianshu.com/p/d2ac26ca6525可以看看更详细的】

2. 版本号控制：一般是在数据表中加上一个数据版本号 version 字段，表示数据被修改的次数。当数据被修改时，version 值会+1。当   线程A要更新数据值时，在读取数据的同时也会读取 version 值，在提交更新时，若刚才读取到的 version 值与当前数据库中的 version 值相等时才更新，否则**重试**更新操作，直到更新成功。

#### 悲观锁(先锁定、后修改)

##### **定义**

​		悲观锁假设数据一定是会造成冲突的，当要对数据库中的一条数据进行**update**的时候，为了避免同时被其他人修改，最好的办法就是直接对该数据进行加锁以防止并发。

​		这种**借助数据库锁机制**，在修改数据之前**先锁定**，**再修改**的方式被称之为悲观并发控制；

​		之所以叫做悲观锁，是因为这是一种对数据的修改持有悲观态度的并发控制方式。总是假设最坏的情况，每次读取数据的时候都默认其他线程会更改数据，因此需要进行加锁操作，当其他线程想要访问数据时，都需要阻塞挂起

##### **实现**

**悲观锁主要分为[共享锁和排他锁](https://www.jianshu.com/p/b4731a7d255a)：**

​		共享锁【shared locks】又称为读锁，简称S锁。顾名思义，共享锁就是**多个事务**对于**同一**数据可以共享**一把锁**，都能访问到数据，但是**只能读不能修改**，只有第一个加了锁的事务才可以进行crud，其他事务只能read。

​		排他锁【exclusive locks】又称为写锁，简称X锁。顾名思义，排他锁就是不能与其他锁并存，如果一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的任何锁，但是获取排他锁的事务是可以对数据行读取和修改。

##### 好处

​		数据安全：悲观并发控制实际上是**“先取锁再访问”**的保守策略，为数据处理的安全提供了保证。

##### 缺点

1. **效率**方面，处理加锁的机制会让数据库产生额外的开销，还有增加产生死锁的机会。

2. 降低**并行性**，一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以处理那行数据。

#### 如何选择乐观锁和悲观锁

在乐观锁与悲观锁的选择上面，主要看下两者的区别以及适用场景就可以了。
 1️⃣响应效率：如果需要非常高的响应速度，建议采用乐观锁方案，成功就执行，不成功就失败，不需要等待其他并发去释放锁。乐观锁并未真正加锁，效率高。一旦锁的粒度掌握不好，更新失败的概率就会比较高，容易发生业务失败。
 2️⃣冲突频率：如果冲突频率非常高，建议采用悲观锁，保证成功率。冲突频率大，选择乐观锁会需要多次重试才能成功，代价比较大。
 3️⃣重试代价：如果重试代价大，建议采用悲观锁。悲观锁依赖数据库锁，效率低。更新失败的概率比较低。
 4️⃣乐观锁如果有人在你之前更新了，你的更新应当是被拒绝的，可以让用户从新操作。悲观锁则会等待前一个更新完成。这也是区别

### CAS

https://www.jianshu.com/p/d2ac26ca6525

# 数据库事务

A(Atomic)原子性：一个事务中的语句要么全部执行，要么全部不执行

C(Consisteny)一致性：以银行转账来说，A给B转账，无论失败与否，A和B的总金额是不变的

I(Isolution)隔离性：一个事务的执行不会影响另一个事务

D(Duration)持久性：一个事务一旦提交，那么他对数据库的影响永久性的，即便发生数据库故障，也是可以恢复的【redoLog】

## 事务隔离级别以及各级别下的并发访问问题

### 并发访问问题有哪些？

- **脏读（Dirty read）:** A事务读取到了B事务尚未commit的数据。
- **丢失修改（Lost to modify）:** A事务对id=1的数据行的修改，被B事务的修改覆盖了，最终只保留了B的修改
- **不可重复读（Unrepeatableread）:** A事务在对id=1数据行两次读取的时候，因为B事务对id=1数据行发生update并commit，导致A事务两次读取的结果不一样
- **幻读（Phantom read）:** 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。

> **不可重复读和幻读区别：**
>
> **不可重复读的重点是update，比如多次读取一条记录发现其中某些列的值被修改**
>
> **幻读的重点在于insert或者delete，比如多次读取一条记录发现记录增多或减少了。**

### 事务的隔离级别有哪些？Mysql默认的隔离级别是?

**SQL 标准定义了四个隔离级别：**

- **READ-UNCOMMITTED(读取未提交)：** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**。
- **READ-COMMITTED(读取已提交)：** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**。
- **REPEATABLE-READ(可重复读)：** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生**。
- **SERIALIZABLE(可串行化)：** 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。

| 隔离级别         | 脏读 | 不可重复读 | 幻影读 |
| ---------------- | ---- | ---------- | ------ |
| READ-UNCOMMITTED | √    | √          | √      |
| READ-COMMITTED   | ×    | √          | √      |
| REPEATABLE-READ  | ×    | ×          | √      |
| SERIALIZABLE     | ×    | ×          | ×      |

​		MySQL InnoDB 存储引擎的默认支持的隔离级别是 **REPEATABLE-READ（可重读）**。我们可以通过`SELECT @@tx_isolation;`命令来查看，MySQL 8.0 该命令改为`SELECT @@transaction_isolation;`

```sql
mysql> SELECT @@tx_isolation;
+-----------------+
| @@tx_isolation  |
+-----------------+
| REPEATABLE-READ |
+-----------------+
```

这里需要注意的是：

1. 与 SQL 标准不同的地方在于 InnoDB 存储引擎在 **REPEATABLE-READ（可重读）** 事务隔离级别下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生
2. InnoDB 存储引擎在 **分布式事务** 的情况下一般会用到 **SERIALIZABLE(可串行化)** 隔离级别。

### InnoDB的RR级别下如何避免幻读？

表象：快照度（非阻塞读）--伪MVCC【不是实际原因】

内在：next-key lock锁（行锁+gap锁）【实际原因】

> 行锁：
>
> gap锁：当隔离级别>=RR的时候才有gap锁

**对主键索引 或者 唯一索引会使用gap锁吗？**

当where条件中使用到了主键索引 或者 唯一索引，

如果where条件全部命中，则只会加行锁：record lock

如果where条件部分命中或者全不命中，则会加gap

比如：

select * from test_table where id in(1,2,7)，如果1，2，7都存在，则只会把id=1，2，7的行加锁；

如果1，2，7中7不存在，会把id=1到id=7的行加gap lock，此时如果有其他事务往1-7中间插入或者修改则必须等待前一个事务结束

**对非唯一索引或者不走索引**

非唯一索引：会把查出来的数据进行gap加锁

不走索引：表锁

#### 当前读和快照读

当前读：select....lock in share mode ； select.....for update【因为别的事务已经无法对加锁数据进行修改，所以当前读是指读取本事务中最新的数据】

当前读：update，delete，insert【因为别的事务已经无法对加锁数据进行修改，所以当前读是指读取本事务中最新的数据】

快照读：**快照读也叫非阻塞读，即所谓快照读就是不加锁的非阻塞读，就是我们最简单的 `select`操作**,当然了，这里不加锁的非阻塞读是以事务隔离级别不为RR的前提下成立，因为在最高隔离级别下，快照读也会变成当前读，在其后自动加`lock in share mode`

### RC、RR级别下的InnoDB的非阻塞读是如何实现的

1. 数据行里的DB_TRX_ID【事务id】、DB_ROLL_PTR【回滚指针】、DB_ROW_ID【隐藏行id】字段

2. undo日志

   <img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171616.png" alt="image-20210302223045591" style="zoom:33%;" />

   ​					undo log记录的是修改前的数据行，

   ​					比如当版本1的12修改为版本2  32的时候，则将版本1数据行记录到undolog日志中

   ​					比如当版本2的13修改为版本2  45的时候，则将版本2数据行记录到undolog日志中,并且通过DB_ROLL_PTR连接起来

   3. read view：用来做可见性判断，当我们执行快照读select的时候，通过read view来决定快照读可以读到哪个版本的数据，有可能是最新的日志，也有可能是undo log中的日志

      > read view内部算法：通过比较DB_TRX_ID,来决定DB_ROLL_PTR的版本，最终通过DB_ROLL_PTR来找到对应的版本

   ​          在RR级别下，在start transation之后会创建一个快照即read view，将当前系统中活跃的事务记录起来，此后调用快照读的时候还是第一次创建的read view；如果A事务执行select快照读先于B事务的update，则A事务的快照读永远读不到B事务update提交后的数据，但是A事务的共享读锁【lock in read model】还是可以读取到的；而如果B事务Update提交之后，A事务才执行select快照读，那么是可以读取到B事务Update的最新数据;

   ​			RR【Repeatable Read】隔离级别下，当前读返回的是数据的最新版本，而快照读在该隔离级别下可能读到数据的历史版本.在 RR 隔离级别下，事务首次调用快照读的时机很关键，即创造快照的时机决定了快照的版本
   
   ​          而在RC级别下，在start transation之后每次进行快照读的时候，都会创建一个新的快照，读取最新的数据

# Other Q

## 解释一下什么是池化设计思想。什么是数据库连接池?为什么要数据库连接池?

​		池化设计应该不是一个新名词。我们常见的如java线程池、jdbc连接池、redis连接池等就是这类设计的代表实现。

​		数据库连接本质就是一个 socket 的连接。数据库服务端还要维护一些缓存和用户权限信息之类的 所以占用了一些内存。我们可以把数据库连接池是看做是维护的数据库连接的缓存，以便将来需要对数据库的请求时可以重用这些连接。为每个用户打开和维护数据库连接，尤其是对动态数据库驱动的网站应用程序的请求，既昂贵又浪费资源。**在连接池中，创建连接后，将其放置在池中，并再次使用它，因此不必建立新的连接。如果使用了所有连接，则会建立一个新连接并将其添加到池中**。 连接池还减少了用户必须等待建立与数据库的连接的时间。

1． 资源重用
由于数据库连接得到重用，避免了频繁创建、释放连接引起的大量性能开销。在减少系统消耗的基础上，另一方面也增进了系统运行环境的平稳性（减少内存碎片以及数据库临时进程/线程的数量）。
2． 更快的系统响应速度
数据库连接池在初始化过程中，往往已经创建了若干数据库连接置于池中备用。此时连接的初始化工作均已完成。对于业务请求处理而言，直接利用现有可用连接，避免了数据库连接初始化和释放过程的时间开销，从而缩减了系统整体响应时间。
3． 新的资源分配手段
对于多应用共享同一数据库的系统而言，可在应用层通过数据库连接的配置，使用数据库连接池技术。设置某一应用最大可用数据库连接数，避免某一应用独占所有数据库资源。
4． 统一的连接管理，避免数据库连接泄漏
在较为完备的数据库连接池实现中，可根据预先设定的连接占用超时时间，强制收回被超时占用的连接。从而避免了常规数据库连接操作中可能出现的资源泄漏（当程序存在缺陷时，申请的连接忘记关闭，这时候，就存在连接泄漏了）。

## 大表如何优化

> 详细内容可以参考： MySQL大表优化方案: https://segmentfault.com/a/1190000006158186

当MySQL单表记录数过大时，增删改查性能都会急剧下降，可以参考以下步骤来优化：

### 单表优化

#### 1.字段规范

##### 1.1 优先选择符合存储需要的最小的数据类型

###### 原因

​		列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的 IO 次数也就越多，索引的性能也就越差。

###### 方法

**a.将字符串转换成数字类型存储,如:将 IP 地址转换成整形数据**

MySQL 提供了两个方法来处理 ip 地址

- inet_aton 把 ip 转为无符号整型 (4-8 位)
- inet_ntoa 把整型的 ip 转为地址

插入数据前，先用 inet_aton 把 ip 地址转为整型，可以节省空间，显示数据时，使用 inet_ntoa 把整型的 ip 地址转为地址显示即可。

**b.对于非负型的数据 (如自增 ID,整型 IP) 来说,要优先使用无符号整型来存储**

​	**原因：**	无符号相对于有符号可以多出一倍的存储空间	

```
SIGNED INT -2147483648~2147483647
UNSIGNED INT 0~4294967295
```

VARCHAR(N) 中的 N 代表的是字符数，而不是字节数，使用 UTF8 存储 255 个汉字 Varchar(255)=765 个字节。**过大的长度会消耗更多的内存**

##### 1.2 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据

**a. 建议把 BLOB 或是 TEXT 列分离到单独的扩展表中**

MySQL 内存临时表不支持 TEXT、BLOB 这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，MySQL 还是要进行二次查询，会使 sql 性能变得很差，但是不是说一定不能使用这样的数据类型。

如果一定要使用，建议把 BLOB 或是 TEXT 列分离到单独的扩展表中，查询时一定不要使用 select * 而只需要取出必要的列，不需要 TEXT 列的数据时不要对该列进行查询。

**2、TEXT 或 BLOB 类型只能使用前缀索引**

因为[MySQL](http://mp.weixin.qq.com/s?__biz=MzI4Njc5NjM1NQ==&mid=2247487885&idx=1&sn=65b1bf5f7d4505502620179669a9c2df&chksm=ebd62ea1dca1a7b7bf884bcd9d538d78ba064ee03c09436ca8e57873b1d98a55afd6d7884cfc&scene=21#wechat_redirect) 对索引字段长度是有限制的，所以 TEXT 类型只能使用前缀索引，并且 TEXT 列上是不能有默认值的

##### 1.3尽可能把所有列定义为 NOT NULL

**原因：**

1. 索引 NULL 列需要额外的空间来保存，所以要占用更多的空间

2. 进行比较和计算时要对 NULL 值做特别的处理



#### 2.索引规范

##### [1. 限制每张表上的索引数量,建议单张表索引不超过 5 个](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_1-限制每张表上的索引数量建议单张表索引不超过-5-个)

索引并不是越多越好！索引可以提高效率同样可以降低效率。

索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。

因为 MySQL 优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加 MySQL 优化器生成执行计划的时间，同样会降低查询性能。

##### [2. 禁止给表中的每一列都建立单独的索引](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_2-禁止给表中的每一列都建立单独的索引)

5.6 版本之前，一个 sql 只能使用到一个表中的一个索引，5.6 以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好。

##### [3. 每个 Innodb 表必须有个主键](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_3-每个-innodb-表必须有个主键)

Innodb 是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。

Innodb 是按照主键索引的顺序来组织表的

- 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引）
- 不要使用 UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长）
- 主键建议使用自增 ID 值

------

##### [4. 常见索引列建议](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_4-常见索引列建议)

- 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列
- 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段
- 并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好
- 多表 join 的关联列

------

##### [5.如何选择索引列的顺序](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_5如何选择索引列的顺序)

建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。

- 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数）
- 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好）
- 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引）

------

##### [6. 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间）](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_6-避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间）)

- 重复索引示例：primary key(id)、index(id)、unique index(id)
- 冗余索引示例：index(a,b,c)、index(a,b)、index(a)

------

##### [7. 对于频繁的查询优先考虑使用覆盖索引](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_7-对于频繁的查询优先考虑使用覆盖索引)

> 覆盖索引：就是包含了所有查询字段 (where,select,ordery by,group by 包含的字段) 的索引

**覆盖索引的好处：**

- **避免 Innodb 表进行索引的二次查询:** Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。
- **可以把随机 IO 变成顺序 IO 加快查询效率:** 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。

------

##### [8.索引 SET 规范](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_8索引-set-规范)

**尽量避免使用外键约束**

- 不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引
- 外键可用于保证数据的参照完整性，但建议在业务端实现
- 外键会影响父表和子表的写操作从而降低性能

#### 3.sql书写规范

##### [1. 建议使用预编译语句进行数据库操作](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_1-建议使用预编译语句进行数据库操作)

预编译语句可以重复使用这些计划，减少 SQL 编译所需要的时间，还可以解决动态 SQL 所带来的 SQL 注入的问题。

只传参数，比传递 SQL 语句更高效。相同语句可以一次解析，多次使用，提高处理效率。

**MyBatis预编译机制详解**

> https://blog.csdn.net/baidu_36030459/article/details/75566734?utm_source=blogxgwz7
>
> https://blog.csdn.net/weixin_34452850/article/details/88991943
>
> https://www.cnblogs.com/linjiaxin/p/6101609.html
>
> https://blog.csdn.net/bingguang1993/article/details/102696877

一. "#{}“和”${}"的区别
1."#{}"是将传入的值按照字符串的形式进行处理，如下面这条语句：

``select user_id,user_name from t_user where user_id = #{user_id}``
MyBaits会首先对其进行预编译，将#{user_ids}替换成?占位符，然后在执行时替换成实际传入的user_id值，**并在两边加上单引号，以字符串方式处理。**下面是MyBatis执行日志：

``10:27:20.247 [main] DEBUG william.mybatis.quickstart.mapper.UserMapper.selectById - ==>  Preparing: select id, user_name from t_user where id = ? 
10:27:20.285 [main] DEBUG william.mybatis.quickstart.mapper.UserMapper.selectById - ==> Parameters: 1(Long)``
因为"#{}"会在传入的值两端加上单引号，所以可以很大程度上防止SQL注入。因此在大多数情况下，建议使用"#{}"。

2."${}"是做简单的字符串替换，即将传入的值直接拼接到SQL语句中，且不会自动加单引号。将上面的SQL语句改为：

``select user_id,user_name from t_user where user_id = ${user_id}``

再观察MyBatis的执行日志：

``10:41:32.242 [main] DEBUG william.mybatis.quickstart.mapper.UserMapper.selectById - ==>  Preparing: select id, user_name, real_name, sex, mobile, email, note, position_id from t_user where id = 1 
10:41:32.288 [main] DEBUG william.mybatis.quickstart.mapper.UserMapper.selectById - ==> Parameters:1 ``

可以看到，参数是直接替换的，且没有单引号处理，这样就有SQL注入的风险。

但是在一些特殊情况下，使用${}是更适合的方式，如表名、orderby等。见下面这个例子：

``select user_id,user_name from ${table_name} where user_id = ${user_id}``

这里如果想要动态处理表名，就只能使用"${}"，因为如果使用"#{}"，就会在表名字段两边加上单引号，变成下面这样：

``select user_id,user_name from 't_user' where user_id = ${user_id}``
这样SQL语句就会报错。

##### [3. 充分利用表上已经存在的索引](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_3-充分利用表上已经存在的索引)

避免使用双%号的查询条件。如：`a like '%123%'`，（如果无前置%,只有后置%，是可以用到列上的索引的）

一个 SQL 只能利用到复合索引中的一列进行范围查询。如：有 a,b,c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b,c 列上的索引将不会被用到。

在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧，使用 left join 或 not exists 来优化 not in 操作，因为 not in 也通常会使用索引失效。

##### [6. 禁止使用 SELECT * 必须使用 SELECT <字段列表> 查询](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_6-禁止使用-select-必须使用-select-lt字段列表gt-查询)

**原因：**

- 消耗更多的 CPU 和 IO 以网络带宽资源
- 无法使用覆盖索引
- 可减少表结构变更带来的影响

##### [7. 禁止使用不含字段列表的 INSERT 语句](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_7-禁止使用不含字段列表的-insert-语句)

如：

```
insert into values ('a','b','c');
```

应使用：

```
insert into t(c1,c2,c3) values ('a','b','c');
```

##### [8. 避免使用子查询，可以把子查询优化为 join 操作](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_8-避免使用子查询，可以把子查询优化为-join-操作)

通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。

**子查询性能差的原因：**

子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。

由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。

##### [9. 避免使用 JOIN 关联太多的表](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_9-避免使用-join-关联太多的表)

对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由 join_buffer_size 参数进行设置。

在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大。

如果程序中大量的使用了多表关联的操作，同时 join_buffer_size 设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。

同时对于关联操作来说，会产生临时表操作，影响查询效率，MySQL 最多允许关联 61 个表，建议不超过 5 个。

##### [10. 减少同数据库的交互次数](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_10-减少同数据库的交互次数)

数据库更适合处理批量操作，合并多个相同的操作到一起，可以提高处理效率。

##### [11. 对应同一列进行 or 判断时，使用 in 代替 or](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_11-对应同一列进行-or-判断时，使用-in-代替-or)

in 的值不要超过 500 个，in 操作可以更有效的利用索引，or 大多数情况下很少能利用到索引。

##### [13. WHERE 从句中禁止对列进行函数转换和计算](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_13-where-从句中禁止对列进行函数转换和计算)

对列进行函数转换或计算时会导致无法使用索引

**不推荐：**

```
where date(create_time)='20190101'
```

**推荐：**

```
where create_time >= '20190101' and create_time < '20190102'
```

##### [14. 在明显不会有重复值时使用 UNION ALL 而不是 UNION](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_14-在明显不会有重复值时使用-union-all-而不是-union)

- UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作
- UNION ALL 不会再对结果集进行去重操作

##### [15. 拆分复杂的大 SQL 为多个小 SQL](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL高性能优化规范建议?id=_15-拆分复杂的大-sql-为多个小-sql)

- 大 SQL 逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL
- MySQL 中，一个 SQL 只能使用一个 CPU 进行计算
- SQL 拆分后可以通过并行执行来提高处理效率

#### 4.限定数据的范围

1. 查询时：比如携带范围查找条件。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内；
2. 对历史数据定时归档

#### 5.缓存

缓存可以发生在这些层次：

- MySQL内部：在系统调优参数介绍了相关设置
- 数据访问层：比如MyBatis针对SQL语句做缓存，而Hibernate可以精确到单个记录，这里缓存的对象主要是持久化对象`Persistence Object`
- 应用服务层：这里可以通过编程手段对缓存做到更精准的控制和更多的实现策略，这里缓存的对象是数据传输对象`Data Transfer Object`
- Web层：针对web页面做缓存
- 浏览器客户端：用户端的缓存

可以根据实际情况在一个层次或多个层次结合加入缓存。这里重点介绍下服务层的缓存实现，目前主要有两种方式：

- 直写式（Write Through）：在数据写入数据库后，同时更新缓存，维持数据库与缓存的一致性。这也是当前大多数应用缓存框架如Spring Cache的工作方式。这种实现非常简单，同步好，但效率一般。
- 回写式（Write Back）：当有数据要写入数据库时，只会更新缓存，然后异步批量的将缓存数据同步到数据库上。这种实现比较复杂，需要较多的应用逻辑，同时可能会产生数据库与缓存的不同步，但效率非常高。

#### 6.读写分离

经典的数据库拆分方案，主库负责写，从库负责读；

#### 7.垂直分区

**根据数据库里面数据表的相关性进行拆分。** 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。

**简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。** 如下图所示，这样来说大家应该就更容易理解了。

 <img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303232805.png" alt="image-20210303180729602" style="zoom:40%;" />

- **垂直拆分的优点：** 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。
- **垂直拆分的缺点：** 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；

#### 8.水平分区

​		**保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。**

​		水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303232806.png" alt="image-20210303180823632" style="zoom:50%;" />



​		水平拆分可以支持非常大的数据量。需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 **水平拆分最好分库** 。

​		水平拆分能够 **支持非常大的数据量存储，应用端改造也少**，但 **分片事务难以解决** ，跨节点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 **尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度** ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。

**下面补充一下数据库分片的两种常见方案：**

- **客户端代理：** **分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。** 当当网的 **Sharding-JDBC** （推荐） 、阿里的TDDL是两种比较常用的实现。
- **中间件代理：** **在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。** 我们现在谈的 **Mycat** 、360的Atlas、网易的DDB等等都是这种架构的实现。

## 分库分表之后，主键id如何处理？

因为要是分成多个表之后，每个表都是从 1 开始累加，这样是不对的，我们需要一个全局唯一的 id 来支持。

生成全局 id 有下面这几种方式：

- **UUID**：不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。
- **数据库自增 id** : 两台数据库分别设置不同步长，生成不重复ID的策略来实现高可用。这种方式生成的 id 有序，但是需要独立部署数据库实例，成本高，还会有性能瓶颈。
- **利用 redis 生成 id :** 性能比较好，灵活方便，不依赖于数据库。但是，引入了新的组件造成系统更加复杂，可用性降低，编码更加复杂，增加了系统成本。
- **Twitter的snowflake算法** ：Github 地址：https://github.com/twitter-archive/snowflake。
- **美团的[Leaf](https://tech.meituan.com/2017/04/21/mt-leaf.html)分布式ID生成系统** ：Leaf 是美团开源的分布式ID生成器，能保证全局唯一性、趋势递增、单调递增、信息安全，里面也提到了几种分布式方案的对比，但也需要依赖关系数据库、Zookeeper等中间件。感觉还不错。美团技术团队的一篇文章：https://tech.meituan.com/2017/04/21/mt-leaf.html 。

雪花算法

```
/**
 * Twitter_Snowflake<br>
 * SnowFlake的结构如下(每部分用-分开):<br>
 * 0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000 <br>
 * 1位标识，由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0<br>
 * 41位时间截(毫秒级)，注意，41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截)
 * 得到的值），这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的（如下下面程序IdWorker类的startTime属性）。41位的时间截，可以使用69年，年T = (1L << 41) / (1000L * 60 * 60 * 24 * 365) = 69<br>
 * 10位的数据机器位，可以部署在1024个节点，包括5位datacenterId和5位workerId<br>
 * 12位序列，毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号<br>
 * 加起来刚好64位，为一个Long型。<br>
 * SnowFlake的优点是，整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞(由数据中心ID和机器ID作区分)，并且效率较高，经测试，SnowFlake每秒能够产生26万ID左右。
 */
public class SnowflakeIdWorker {

    // ==============================Fields===========================================
    /** 开始时间截 (2015-01-01) */
    private final long twepoch = 1420041600000L;

    /** 机器id所占的位数 */
    private final long workerIdBits = 5L;

    /** 数据标识id所占的位数 */
    private final long datacenterIdBits = 5L;

    /** 支持的最大机器id，结果是31 (这个移位算法可以很快的计算出几位二进制数所能表示的最大十进制数) */
    private final long maxWorkerId = -1L ^ (-1L << workerIdBits);

    /** 支持的最大数据标识id，结果是31 */
    private final long maxDatacenterId = -1L ^ (-1L << datacenterIdBits);

    /** 序列在id中占的位数 */
    private final long sequenceBits = 12L;

    /** 机器ID向左移12位 */
    private final long workerIdShift = sequenceBits;

    /** 数据标识id向左移17位(12+5) */
    private final long datacenterIdShift = sequenceBits + workerIdBits;

    /** 时间截向左移22位(5+5+12) */
    private final long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;

    /** 生成序列的掩码，这里为4095 (0b111111111111=0xfff=4095) */
    private final long sequenceMask = -1L ^ (-1L << sequenceBits);

    /** 工作机器ID(0~31) */
    private long workerId;

    /** 数据中心ID(0~31) */
    private long datacenterId;

    /** 毫秒内序列(0~4095) */
    private long sequence = 0L;

    /** 上次生成ID的时间截 */
    private long lastTimestamp = -1L;

    //==============================Constructors=====================================
    /**
     * 构造函数
     * @param workerId 工作ID (0~31)
     * @param datacenterId 数据中心ID (0~31)
     */
    public SnowflakeIdWorker(long workerId, long datacenterId) {
        if (workerId > maxWorkerId || workerId < 0) {
            throw new IllegalArgumentException(String.format("worker Id can't be greater than %d or less than 0", maxWorkerId));
        }
        if (datacenterId > maxDatacenterId || datacenterId < 0) {
            throw new IllegalArgumentException(String.format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId));
        }
        this.workerId = workerId;
        this.datacenterId = datacenterId;
    }

//1000000000000
// 111111111111
//1000000000001
    // ==============================Methods==========================================
    /**
     * 获得下一个ID (该方法是线程安全的)
     * @return SnowflakeId
     */
    public synchronized long nextId() {
        long timestamp = timeGen();

        //如果当前时间小于上一次ID生成的时间戳，说明系统时钟回退过这个时候应当抛出异常
        if (timestamp < lastTimestamp) {
            throw new RuntimeException(
                    String.format("Clock moved backwards.  Refusing to generate id for %d milliseconds", lastTimestamp - timestamp));
        }

        //如果是同一时间生成的，则进行毫秒内序列
        if (lastTimestamp == timestamp) {
            sequence = (sequence + 1) & sequenceMask;
            //毫秒内序列溢出
            if (sequence == 0) {
                //阻塞到下一个毫秒,获得新的时间戳
                timestamp = tilNextMillis(lastTimestamp);
            }
        }
        //时间戳改变，毫秒内序列重置
        else {
            sequence = 0L;
        }

        //上次生成ID的时间截
        lastTimestamp = timestamp;

        //移位并通过或运算拼到一起组成64位的ID
        return ((timestamp - twepoch) << timestampLeftShift) //
                | (datacenterId << datacenterIdShift) //
                | (workerId << workerIdShift) //
                | sequence;
    }

    /**
     * 阻塞到下一个毫秒，直到获得新的时间戳
     * @param lastTimestamp 上次生成ID的时间截
     * @return 当前时间戳
     */
    protected long tilNextMillis(long lastTimestamp) {
        long timestamp = timeGen();
        while (timestamp <= lastTimestamp) {
            timestamp = timeGen();
        }
        return timestamp;
    }

    /**
     * 返回以毫秒为单位的当前时间
     * @return 当前时间(毫秒)
     */
    protected long timeGen() {
        return System.currentTimeMillis();
    }

    //==============================Test=============================================
    /** 测试 */
    public static void main(String[] args) {
        SnowflakeIdWorker idWorker = new SnowflakeIdWorker(0, 0);
        for (int i = 0; i < 10; i++) {
            long id = idWorker.nextId();
            System.out.println(Long.toBinaryString(id));
            System.out.println(id);
        }
    }
}
```



## 一条sql是如何执行的

> 参考：https://snailclimb.gitee.io/javaguide/#/docs/database/%E4%B8%80%E6%9D%A1sql%E8%AF%AD%E5%8F%A5%E5%9C%A8mysql%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84

### 一、mysql基础架构分析

#### 1.1 MySQL 基本架构概览

下图是 MySQL 的一个简要架构图，从下图你可以很清晰的看到用户的 SQL 语句在 MySQL 内部是如何执行的。

先简单介绍一下下图涉及的一些组件的基本作用帮助大家理解这幅图，在 1.2 节中会详细介绍到这些组件的作用。

- **连接器：** 身份认证和权限相关(登录 MySQL 的时候)。
- **查询缓存:** 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。
- **分析器:** 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。
- **优化器：** 按照 MySQL 认为最优的方案去执行。
- **执行器:** 执行语句，然后从存储引擎返回数据。

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303232807.png" alt="image-20210303220618494" style="zoom:35%;" />

简单来说 MySQL 主要分为 Server 层和存储引擎层：

- **Server 层**：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。
- **存储引擎**： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。**现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始就被当做默认存储引擎了。**

#### [1.2 Server 层基本组件介绍](https://snailclimb.gitee.io/javaguide/#/docs/database/一条sql语句在mysql中如何执行的?id=_12-server-层基本组件介绍)

##### [1) 连接器](https://snailclimb.gitee.io/javaguide/#/docs/database/一条sql语句在mysql中如何执行的?id=_1-连接器)

连接器主要和身份认证和权限相关的功能相关，就好比一个级别很高的门卫一样。

主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作，如果用户账户密码已通过，连接器会到权限表中查询该用户的所有权限，之后在这个连接里的权限逻辑判断都是会依赖此时读取到的权限数据，也就是说，后续只要这个连接不断开，即时管理员修改了该用户的权限，该用户也是不受影响的。

##### [2) 查询缓存(MySQL 8.0 版本后移除)](https://snailclimb.gitee.io/javaguide/#/docs/database/一条sql语句在mysql中如何执行的?id=_2-查询缓存mysql-80-版本后移除)

查询缓存主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集。

连接建立后，执行查询语句的时候，会先查询缓存，MySQL 会先校验这个 sql 是否执行过，以 Key-Value 的形式缓存在内存中，Key 是查询预计，Value 是结果集。如果缓存 key 被命中，就会直接返回给客户端，如果没有命中，就会执行后续的操作，完成后也会把结果缓存起来，方便下一次调用。当然在真正执行缓存查询的时候还是会校验用户的权限，是否有该表的查询条件。

MySQL 查询不建议使用缓存，因为查询缓存失效在实际业务场景中可能会非常频繁，假如你对一个表更新的话，这个表上的所有的查询缓存都会被清空。对于不经常更新的数据来说，使用缓存还是可以的。

所以，一般在大多数情况下我们都是不推荐去使用查询缓存的。

MySQL 8.0 版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了。

##### [3) 分析器](https://snailclimb.gitee.io/javaguide/#/docs/database/一条sql语句在mysql中如何执行的?id=_3-分析器)

MySQL 没有命中缓存，那么就会进入分析器，分析器主要是用来分析 SQL 语句是来干嘛的，分析器也会分为几步：

**第一步，词法分析**，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。

**第二步，语法分析**，主要就是判断你输入的 sql 是否正确，是否符合 MySQL 的语法。

完成这 2 步之后，MySQL 就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。

##### [4) 优化器](https://snailclimb.gitee.io/javaguide/#/docs/database/一条sql语句在mysql中如何执行的?id=_4-优化器)

优化器的作用就是它认为的最优的执行方案去执行（有时候可能也不是最优，这篇文章涉及对这部分知识的深入讲解），比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。

可以说，经过了优化器之后可以说这个语句具体该如何执行就已经定下来。

##### [5) 执行器](https://snailclimb.gitee.io/javaguide/#/docs/database/一条sql语句在mysql中如何执行的?id=_5-执行器)

当选择了执行方案后，MySQL 就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。

### [二 语句分析](https://snailclimb.gitee.io/javaguide/#/docs/database/一条sql语句在mysql中如何执行的?id=二-语句分析)

#### [2.1 查询语句](https://snailclimb.gitee.io/javaguide/#/docs/database/一条sql语句在mysql中如何执行的?id=_21-查询语句)

说了以上这么多，那么究竟一条 sql 语句是如何执行的呢？其实我们的 sql 可以分为两种，一种是查询，一种是更新（增加，更新，删除）。我们先分析下查询语句，语句如下：

```sql
select * from tb_student  A where A.age='18' and A.name=' 张三 ';Copy to clipboardErrorCopied
```

结合上面的说明，我们分析下这个语句的执行流程：

- 先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在 MySQL8.0 版本以前，会先查询缓存，以这条 sql 语句为 key 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。

- 通过分析器进行词法分析，提取 sql 语句的关键元素，比如提取上面这个语句是查询 select，提取需要查询的表名为 tb_student,需要查询所有的列，查询条件是这个表的 id='1'。然后判断这个 sql 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。

- 接下来就是优化器进行确定执行方案，上面的 sql 语句，可以有两种执行方案：

  ```markup
    a.先查询学生表中姓名为“张三”的学生，然后判断是否年龄是 18。
    b.先找出学生中年龄 18 岁的学生，然后再查询姓名为“张三”的学生。Copy to clipboardErrorCopied
  ```

  那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了。

- 进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。

#### [2.2 更新语句](https://snailclimb.gitee.io/javaguide/#/docs/database/一条sql语句在mysql中如何执行的?id=_22-更新语句)

以上就是一条查询 sql 的执行流程，那么接下来我们看看一条更新语句如何执行的呢？sql 语句如下：

```
update tb_student A set A.age='19' where A.name=' 张三 ';Copy to clipboardErrorCopied
```

我们来给张三修改下年龄，在实际数据库肯定不会设置年龄这个字段的，不然要被技术负责人打的。其实条语句也基本上会沿着上一个查询的流程走，只不过执行更新的时候肯定要记录日志啦，这就会引入日志模块了，MySQL 自带的日志模块式 **binlog（归档日志）** ，所有的存储引擎都可以使用，我们常用的 InnoDB 引擎还自带了一个日志模块 **redo log（重做日志）**，我们就以 InnoDB 模式下来探讨这个语句的执行流程。流程如下：

- 先查询到张三这一条数据，如果有缓存，也是会用到缓存。
- 然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。
- 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。
- 更新完成。

**这里肯定有同学会问，为什么要用两个日志模块，用一个日志模块不行吗?**

这是因为最开始 MySQL 并没与 InnoDB 引擎( InnoDB 引擎是其他公司以插件形式插入 MySQL 的) ，MySQL 自带的引擎是 MyISAM，但是我们知道 redo log 是 InnoDB 引擎特有的，其他存储引擎都没有，这就导致会没有 crash-safe 的能力(crash-safe 的能力即使数据库发生异常重启，之前提交的记录都不会丢失)，binlog 日志只能用来归档。

并不是说只用一个日志模块不可以，只是 InnoDB 引擎就是通过 redo log 来支持事务的。那么，又会有同学问，我用两个日志模块，但是不要这么复杂行不行，为什么 redo log 要引入 prepare 预提交状态？这里我们用反证法来说明下为什么要这么做？

- **先写 redo log 直接提交，然后写 binlog**，假设写完 redo log 后，机器挂了，binlog 日志没有被写入，那么机器重启后，这台机器会通过 redo log 恢复数据，但是这个时候 bingog 并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。
- **先写 binlog，然后写 redo log**，假设写完了 binlog，机器异常重启了，由于没有 redo log，本机是无法恢复这一条记录的，但是 binlog 又有记录，那么和上面同样的道理，就会产生数据不一致的情况。

如果采用 redo log 两阶段提交的方式就不一样了，写完 binglog 后，然后再提交 redo log 就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设 redo log 处于预提交状态，binglog 也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于 MySQL 的处理机制了，MySQL 的处理过程如下：

- 判断 redo log 是否完整，如果判断是完整的，就立即提交。
- 如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log, 不完整就回滚事务。

这样就解决了数据一致性的问题。

### [三 总结](https://snailclimb.gitee.io/javaguide/#/docs/database/一条sql语句在mysql中如何执行的?id=三-总结)

- MySQL 主要分为 Server 层和引擎层，Server 层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用,redolog 只有 InnoDB 有。
- 引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory 等。
- 查询语句的执行流程如下：权限校验（如果命中缓存）---》查询缓存---》分析器---》优化器---》权限校验---》执行器---》引擎
- 更新语句执行流程如下：分析器----》权限校验----》执行器---》引擎---redo log(prepare 状态---》binlog---》redo log(commit状态)



## 一条SQL语句执行得很慢的原因有哪些？

> https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247485185&idx=1&sn=66ef08b4ab6af5757792223a83fc0d45&chksm=cea248caf9d5c1dc72ec8a281ec16aa3ec3e8066dbb252e27362438a26c33fbe842b0e0adf47&token=79317275&lang=zh_CN#rd

分两种情况：

**1、大多数情况是正常的，只是偶尔会出现很慢的情况。**

**2、在数据量不变的情况下，这条SQL语句一直以来都执行的很慢。**



### 一、大多数情况是正常的，只是偶尔会出现很慢的情况

#### 1、数据库在刷新脏页（flush）

当我们要往数据库插入一条数据、或者要更新一条数据的时候，我们知道数据库会在<span style='color:orange'>内存</span>中把对应字段的数据更新了，但是更新之后，这些更新的字段并不会马上同步持久化到**磁盘**中去，而是把这些更新的记录写入到 redo log 日记中去，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到**磁盘**中去。

> 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。

**刷脏页有下面4种场景（后两种不用太关注“性能”问题）：**

- **redolog写满了：**redo log 里的容量是有限的，如果数据库一直很忙，更新又很频繁，这个时候 redo log 很快就会被写满了，这个时候就没办法等到空闲的时候再把数据同步到磁盘的，只能暂停其他操作，全身心来把数据同步到磁盘中去的，而这个时候，**就会导致我们平时正常的SQL语句突然执行的很慢**，所以说，数据库在在同步数据到磁盘的时候，就有可能导致我们的SQL语句执行的很慢了。
- **内存不够用了：**如果一次查询较多的数据，恰好碰到所查数据页不在内存中时，需要申请内存，而此时恰好内存不足的时候就需要淘汰一部分内存数据页，如果是干净页，就直接释放，如果恰好是脏页就需要刷脏页。
- **MySQL 认为系统“空闲”的时候：**这时系统没什么压力。
- **MySQL 正常关闭的时候：**这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快

#### 2、拿不到锁

​		操作的数据行被加锁了

​		如果要判断是否真的在等待锁，我们可以用 **show processlist**这个命令来查看当前的状态哦，这里我要提醒一下，有些命令最好记录一下，反正，我被问了好几个命令，都不知道怎么写，呵呵。

### 二、一直以来都执行的很慢

我们先来假设我们有一个表，表里有下面两个字段,分别是主键 id，和两个普通字段 c 和 d。

```sql
mysql> CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  `d` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
```

#### 1.没用索引

没有用上索引，我觉得这个原因是很多人都能想到的，例如你要查询这条语句

```
select * from t where 100 <c and c < 100000;
```

**（1）字段没有索引**

 c 字段上没有索引，只能走全表扫描了

**（2）没有用索引：字段有索引，但却因为操作没有用索引**

比如：假设c有索引，但是对对索引列 c  进行数学操作

```
select * from t where c - 1 = 1000;
```

**（3）没有用索引：函数操作导致没有用上索引**

如果我们在查询的时候，对字段进行了函数操作，也是会导致没有用上索引的，例如

```
select * from t where unix_timestamp(c) = 145728936282;
```

#### 2.数据库选错了索引

我们在进行查询操作的时候，例如

```
select * from t where 100 < c and c < 100000;
```

​		我们知道，主键索引和非主键索引是有区别的，主键索引存放的值是**整行字段的数据**，而非主键索引上存放的值不是整行字段的数据，而且存放**主键字段的值**。也就是说，我们如果走 c 这个字段的索引的话，最后会查询到对应主键的值，然后回表查行数据；

​		好吧扯了这么多，其实我就是想告诉你，就算你在 c 字段上有索引，系统也并不一定会走 c 这个字段上的索引，而是有可能会直接扫描扫描全表，找出所有符合 100 < c and c < 100000 的数据。

**为什么会这样呢？**

​		其实是这样的，系统在执行这条语句的时候，会进行预测：究竟是走 c 索引扫描的行数少，还是直接扫描全表扫描的行数少呢？显然，扫描行数越少当然越好了，因为扫描行数越少，意味着I/O操作的次数越少。

​		如果是扫描全表的话，那么扫描的次数就是这个表的总行数了，假设为 n；而如果走索引 c 的话，我们通过索引 c 找到主键之后，还得再通过主键索引来找我们整行的数据，也就是说，需要走两次索引。而且，我们也不知道符合 100 c < and c < 10000 这个条件的数据有多少行，万一这个表是全部数据都符合呢？这个时候意味着，走 c 索引不仅扫描的行数是 n，同时还得每行数据走两次索引。

**所以呢，系统是有可能走全表扫描而不走索引的。那系统是怎么判断呢？**

​		判断来源于系统的**预测**，也就是说，如果要走 c 字段索引的话，系统会预测走 c 字段索引大概需要扫描多少行。如果预测到要扫描的行数很多，它可能就不走索引而直接扫描全表了。

那么问题来了，**系统是怎么预测判断的呢？**

​		系统是通过**索引的区分度**来判断的，一个索引上不同的值越多，意味着出现相同数值的索引越少，意味着索引的区分度越高。我们也把区分度称之为**基数**，即区分度越高，基数越大。所以呢，一个索引的基数越大，意味着走索引查询越有优势。

**那么问题来了，怎么知道这个索引的基数呢？**

​		系统当然是**不会遍历**全部来获得一个索引的基数的，代价太大了，索引系统是通过遍历部分数据，也就是通过**采样**的方式，来预测索引的基数的。

**扯了这么多，重点的来了**

​		既然是**采样**，那就有可能出现**失误**的情况，也就是说，c 这个索引的基数实际上是很大的，但是采样的时候，却很不幸，把这个索引的基数预测成很小。例如你采样的那一部分数据刚好基数很小，然后就误以为索引的基数很小、区分度不大。**然后就呵呵，系统就不走 c 索引了，直接走全部扫描了**。

所以呢，说了这么多，得出结论

​		**由于统计的失误，导致系统没有走索引，而是走了全表扫描**，而这，也是导致我们 SQL 语句执行的很慢的原因。

> 这里我声明一下，系统判断是否走索引，扫描行数的预测其实只是原因之一，这条查询语句是否需要使用使用临时表、是否需要排序等也是会影响系统的选择的。

不过呢，我们有时候也可以通过强制走索引的方式来查询，例如

```
select * from t force index(a) where c < 100 and c < 100000;
```

我们也可以通过

```
show index from t;
```

来查询索引的基数和实际是否符合，如果和实际很不符合的话，我们可以重新来统计索引的基数，可以用这条命令

```
analyze table t;
```

来重新统计分析。

**既然会预测错索引的基数，这也意味着，当我们的查询语句有多个索引的时候，系统有可能也会选错索引哦**，这也可能是 SQL 执行的很慢的一个原因。

**四、总结**

以上是我的总结与理解，最后一个部分，我怕很多人不大懂**数据库居然会选错索引**，所以我详细解释了一下，下面我对以上做一个总结。

一个 SQL 执行的很慢，我们要分两种情况讨论：

1、大多数情况下很正常，偶尔很慢，则有如下原因

(1)、数据库在刷新脏页，例如 redo log 写满了需要同步到磁盘。

(2)、执行的时候，遇到锁，如表锁、行锁。

2、这条 SQL 语句一直执行的很慢，则有如下原因。

(1)、没有用上索引：例如该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。

(2)、数据库选错了索引。



## 如何存储时间

每种方式都有各自的优势，根据实际场景才是王道。下面再对这三种方式做一个简单的对比，以供大家实际开发中选择正确的存放时间的数据类型：

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210307125325.png" alt="image-20210307125146511" style="zoom:50%;" />

### 切记不要用字符串存储日期

我记得我在大学的时候就这样干过，而且现在很多对数据库不太了解的新手也会这样干，可见，这种存储日期的方式的优点还是有的，就是简单直白，容易上手。

但是，这是不正确的做法，主要会有下面两个问题：

1. 字符串占用的空间更大！
2. 字符串存储的日期比较效率比较低（逐个字符进行比对），无法用日期相关的 API 进行计算和比较。

## redoLog，undoLog，binLog【待补充】

https://www.cnblogs.com/wupeixuan/p/11734501.html

https://www.cnblogs.com/f-ck-need-u/archive/2018/05/08/9010872.html

https://www.imooc.com/article/294391

​		innodb事务日志包括redo log和undo log。redo log是重做日志，提供**前滚**操作，undo log是回滚日志，提供**回滚**操作。

​		 binlog 是 mysql server 层维护的，跟采用何种引擎没有关系，记录的是所有引擎的更新操作的日志记录

### 1. redolog

#### 





### 2. undolog

undo log不是redo log的逆向过程，其实它们都算是用来恢复的日志：

​		**1.redo log通常是物理日志，记录的是数据页的物理修改，而不是某一行或某几行修改成怎样怎样，它用来恢复提交后的物理数据页(恢复数据页，且只能恢复到最后一次提交的位置)。**

​		**2.undo用来回滚行记录到某个版本。undo log一般是逻辑日志，根据每行记录进行记录。**

undo log有两个作用：提供回滚和多个行版本控制(MVCC)。

在数据修改的时候，不仅记录了redo，还记录了相对应的undo，如果因为某些原因导致事务失败或回滚了，可以借助该undo进行回滚。

undo log和redo log记录物理日志不一样，它是逻辑日志。**可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。**

当执行rollback时，就可以从undo log中的逻辑记录读取到相应的内容并进行回滚。有时候应用到行版本控制的时候，也是通过undo log来实现的：当读取的某一行被其他事务锁定时，它可以从undo log中分析出该行记录以前的数据是什么，从而提供该行版本信息，让用户实现非锁定一致性读取。

**undo log****是采用段(segment)****的方式来记录的，每个undo****操作在记录的时候占用一个undo log segment****。**

另外，**undo log****也会产生redo log****，因为undo log****也要实现持久性保护。**





### 3. binlog

binlog记录了对MySQL数据库执行更改的所有操作，但是不包括SELECT和SHOW这类操作，因为这类操作对数据本身并没有修改。然后，若操作本身并没有导致数据库发生变化，那么该操作也会写入二进制日志。例如：

update修改的条数为0时，不记录进binlog中，只有修改条数不为0时才会记录进binlog

## mybatis预编译

https://blog.csdn.net/baidu_36030459/article/details/75566734?utm_source=blogxgwz7

https://blog.csdn.net/weixin_34452850/article/details/88991943

https://www.cnblogs.com/linjiaxin/p/6101609.html

https://blog.csdn.net/bingguang1993/article/details/102696877

### 1. "#{}“和”${}"的区别

#### #{}

"#{}"是将传入的值按照字符串的形式进行处理，如下面这条语句：

````sql
select user_id,user_name from t_user where user_id = #{user_id}
````


​		MyBaits会首先对其进行预编译，将#{user_ids}替换成`?`占位符，然后在执行时替换成实际传入的user_id值，**并在两边加上单引号，以字符串方式处理。**下面是MyBatis执行日志：

````sql
10:27:20.247 [main] DEBUG william.mybatis.quickstart.mapper.UserMapper.selectById - ==>  Preparing: select id, user_name from t_user where id = ? 
10:27:20.285 [main] DEBUG william.mybatis.quickstart.mapper.UserMapper.selectById - ==> Parameters: 1(Long)
````

​		因为"#{}"会在传入的值两端加上单引号，所以可以很大程度上防止SQL注入。有关SQL注入的知识会在后文进行说明。因此在大多数情况下，建议使用"#{}"。

#### ${}

​		"${}"是做简单的字符串替换，即将传入的值直接拼接到SQL语句中，且不会自动加单引号。将上面的SQL语句改为：

````sql
select user_id,user_name from t_user where user_id = ${user_id}
````

再观察MyBatis的执行日志：

````sql
10:41:32.242 [main] DEBUG william.mybatis.quickstart.mapper.UserMapper.selectById - ==>  Preparing: select id, user_name, real_name, sex, mobile, email, note, position_id from t_user where id = 1 
10:41:32.288 [main] DEBUG william.mybatis.quickstart.mapper.UserMapper.selectById - ==> Parameters: 
````


​		可以看到，参数是直接替换的，且没有单引号处理，这样就有SQL注入的风险。

但是在一些特殊情况下，使用${}是更适合的方式，如表名、orderby等。见下面这个例子：

````sql
select user_id,user_name from ${table_name} where user_id = ${user_id}
````

这里如果想要动态处理表名，就只能使用"${}"，因为如果使用"#{}"，就会在表名字段两边加上单引号，变成下面这样：

````sql
select user_id,user_name from 't_user' where user_id = ${user_id}
````

这样SQL语句就会报错。

### 2.源码解析

#### Mybatis对SQL的解析

MyBatis对SQL语句解析的处理在XMLStatementBuilder类中，见源码：

```java
/**
   * 解析mapper中的SQL语句
   */
  public void parseStatementNode() {
    //SQL语句id,对应着Mapper接口的方法
    String id = context.getStringAttribute("id");

    //校验databaseId是否匹配
    String databaseId = context.getStringAttribute("databaseId");
    if (!databaseIdMatchesCurrent(id, databaseId, this.requiredDatabaseId)) {
      return;
    }

    //SQL标签属性解析
    Integer fetchSize = context.getIntAttribute("fetchSize");
    Integer timeout = context.getIntAttribute("timeout");
    String parameterMap = context.getStringAttribute("parameterMap");
    String parameterType = context.getStringAttribute("parameterType");
    Class<?> parameterTypeClass = resolveClass(parameterType);  //参数类型
    String resultMap = context.getStringAttribute("resultMap");
    String resultType = context.getStringAttribute("resultType");
    String lang = context.getStringAttribute("lang");
    LanguageDriver langDriver = getLanguageDriver(lang);

    Class<?> resultTypeClass = resolveClass(resultType);    //结果类型
    String resultSetType = context.getStringAttribute("resultSetType");

    //Statement类型,默认PreparedStatement
    StatementType statementType = StatementType.valueOf(context.getStringAttribute("statementType", StatementType.PREPARED.toString()));
    ResultSetType resultSetTypeEnum = resolveResultSetType(resultSetType);

    String nodeName = context.getNode().getNodeName();
    //SQL命令类型:增删改查
    SqlCommandType sqlCommandType = SqlCommandType.valueOf(nodeName.toUpperCase(Locale.ENGLISH));
    boolean isSelect = sqlCommandType == SqlCommandType.SELECT;
    boolean flushCache = context.getBooleanAttribute("flushCache", !isSelect);
    boolean useCache = context.getBooleanAttribute("useCache", isSelect);
    boolean resultOrdered = context.getBooleanAttribute("resultOrdered", false);

    // Include Fragments before parsing
    XMLIncludeTransformer includeParser = new XMLIncludeTransformer(configuration, builderAssistant);
    includeParser.applyIncludes(context.getNode());

    // Parse selectKey after includes and remove them.
    processSelectKeyNodes(id, parameterTypeClass, langDriver);
    
    // Parse the SQL (pre: <selectKey> and <include> were parsed and removed)
    //重要:解析SQL语句,封装成一个SqlSource
    SqlSource sqlSource = langDriver.createSqlSource(configuration, context, parameterTypeClass);
    String resultSets = context.getStringAttribute("resultSets");
    String keyProperty = context.getStringAttribute("keyProperty");
    String keyColumn = context.getStringAttribute("keyColumn");
    KeyGenerator keyGenerator;
    String keyStatementId = id + SelectKeyGenerator.SELECT_KEY_SUFFIX;
    keyStatementId = builderAssistant.applyCurrentNamespace(keyStatementId, true);
    if (configuration.hasKeyGenerator(keyStatementId)) {
      keyGenerator = configuration.getKeyGenerator(keyStatementId);
    } else {
      keyGenerator = context.getBooleanAttribute("useGeneratedKeys",
          configuration.isUseGeneratedKeys() && SqlCommandType.INSERT.equals(sqlCommandType))
          ? Jdbc3KeyGenerator.INSTANCE : NoKeyGenerator.INSTANCE;
    }

    //解析完毕,最后通过MapperBuilderAssistant创建MappedStatement对象,统一保存到Configuration的mappedStatements属性中
    builderAssistant.addMappedStatement(id, sqlSource, statementType, sqlCommandType,
        fetchSize, timeout, parameterMap, parameterTypeClass, resultMap, resultTypeClass,
        resultSetTypeEnum, flushCache, useCache, resultOrdered, 
        keyGenerator, keyProperty, keyColumn, databaseId, langDriver, resultSets);
  }

```

​		前面是对SQL标签的一些处理，如id、缓存、结果集映射等。

​		我们这次主要分析预编译机制，因此重点关注**SqlSource** sqlSource = langDriver.createSqlSource(configuration, context, parameterTypeClass)这个方法。这该方法会通过LanguageDriver对SQL语句进行解析，生成一个SqlSource。SqlSource封装了映射文件或者注解中定义的SQL语句，它不能直接交给数据库执行，因为里面可能包含动态SQL或者占位符等元素。

​		而MyBatis在实际执行SQL语句时，会调用SqlSource的**getBoundSql()**方法获取一个BoundSql对象，BoundSql是将SqlSource中的动态内容经过处理后，返回的实际可执行的SQL语句，其中包含?占位符List封装的有序的参数映射关系，此外还有一些额外信息标识每个参数的属性名称等。

LanguageDriver的默认实现类是XMLLanguageDriver，我们进入到这个方法里面看下：

```java
//创建SqlSource
@Override
public SqlSource createSqlSource(Configuration configuration, XNode script, Class<?> parameterType) {
  //创建XMLScriptBuilder对象
  XMLScriptBuilder builder = new XMLScriptBuilder(configuration, script, parameterType);

  //通过XMLScriptBuilder解析SQL脚本
  return builder.parseScriptNode();
}
```

这里通过XMLScriptBuilder对象的parseScriptNode()方法进行SQL脚本的解析，继续跟进去：

```java
/**
   * 解析SQL脚本
   */
public SqlSource parseScriptNode() {
  //解析动态标签,包括动态SQL和${}。执行后动态SQL和${}已经被解析完毕。
  //此时SQL语句中的#{}还没有处理,#{}会在SQL执行时动态解析
  MixedSqlNode rootSqlNode = parseDynamicTags(context);

  //如果是dynamic的,则创建DynamicSqlSource,否则创建RawSqlSource
  SqlSource sqlSource = null;
  if (isDynamic) {
    sqlSource = new DynamicSqlSource(configuration, rootSqlNode);
  } else {
    sqlSource = new RawSqlSource(configuration, rootSqlNode, parameterType);
  }
  return sqlSource;
}

```

parseScriptNode的功能就是判断该SQL节点是否是动态的，然后根据是否动态返回DynamicSqlSource或

RawSqlSource。是否为动态SQL的判断在parseDynamicTags()方法中：

````java
protected MixedSqlNode parseDynamicTags(XNode node) {
  List<SqlNode> contents = new ArrayList<>();
  NodeList children = node.getNode().getChildNodes();
  for (int i = 0; i < children.getLength(); i++) {
    XNode child = node.newXNode(children.item(i));

    //处理文本节点(SQL语句)
    if (child.getNode().getNodeType() == Node.CDATA_SECTION_NODE || child.getNode().getNodeType() == Node.TEXT_NODE) {
      //把SQL封装到TextSqlNode
      String data = child.getStringBody("");
      TextSqlNode textSqlNode = new TextSqlNode(data);

      //如果包含${},则是dynamic的
      if (textSqlNode.isDynamic()) {
        contents.add(textSqlNode);
        isDynamic = true;
      } else {
        //除了${}外,其他的SQL都是静态的
        contents.add(new StaticTextSqlNode(data));
      }
    } else if (child.getNode().getNodeType() == Node.ELEMENT_NODE) { // issue #628
      String nodeName = child.getNode().getNodeName();
      NodeHandler handler = nodeHandlerMap.get(nodeName);
      if (handler == null) {
        throw new BuilderException("Unknown element <" + nodeName + "> in SQL statement.");
      }
      handler.handleNode(child, contents);
      isDynamic = true;
    }
  }
  return new MixedSqlNode(contents);
}

````

在这个方法中，会对SQL语句进行动态标签的解析。以<select>标签为例，会获取标签中的文本节点(即具体的SQL语句)，将其封装成TextSqlNode，然后调用isDynamic()方法判断是否为动态标签。那么我们来看看这个方法：

```java
public boolean isDynamic() {
  DynamicCheckerTokenParser checker = new DynamicCheckerTokenParser();
  GenericTokenParser parser = createParser(checker);
  parser.parse(text);
  return checker.isDynamic();
}

```

这里涉及一些底层的文本解析，这里就不具体说明了，我们仅需看下createParser()这个方法：

```
private GenericTokenParser createParser(TokenHandler handler) {
  return new GenericTokenParser("${", "}", handler);
}

```

​		这样一来就明白了，该方法会创建一个以"${}“为token的解析器GenericTokenParser，对指定的SQL语句进行解析，如果解析成功，说明语句中包含”${}"，则将其标记为动态SQL标签。如果是动态SQL标签，创建的SqlSource就是DynamicSqlSource，其获取的BoundSql就是直接进行字符串的替换。

​		对于非动态标签，则创建RawSqlSource，对应?占位符的SQL语句，如前文所述。


## count(id),count(*),count(0),count(coluName)的区别

首先摆出结论：**count(可空字段) < count(非空字段) = count(主键 id) < count(1) ≈ count(\*)**

**count(可空字段)**

InnoDB会遍历整张表，返回该字段，server层判断该字段 不为空则累加

**count(非空字段)与count(主键 id)**

InnoDB会遍历整张表 ，把这个非空字段或者主键id取出来，返回给server层，server层拿到非空字段或者主键id后，判断出字段不可能为空，就按行累加

**count(1)**

InnoDB会遍历整张表 ，不取值，按行累加。

注意：count(1)执行速度比count(主键 id)快的原因：从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。

**count(\*)**

InnoDB会遍历整张表 ，不取值，按行累加。

## InnoDB主键生成策略

​		优先使用用户自定义主键作为主键，如果用户没有定义主键，则选取一个`Unique`键作为主键，如果表中连`Unique`键都没有定义的话，则`InnoDB`会为表默认添加一个名为`row_id`的隐藏列作为主键

​		InnoDB存储引擎***row_id*** 是可选的（在没有自定义主键以及Unique键的情况下才会添加该列）

# 理论范式

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171617.png" alt="image-20210302235641126" style="zoom: 33%;" />![image-20210303000133151](https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171618.png)



![image-20210303000133151](https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171618.png)

<img src="https://zhaozihui-typro.oss-cn-beijing.aliyuncs.com/typora/20210303171619.png" alt="image-20210303000340947" style="zoom:33%;" />







































